{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f4af00",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "\n",
    "Here we develop a \"hybrid\" recommender for the Microsoft News Dataset\n",
    "[@microsoft-2020] by means of the\n",
    "[LightFM](https://making.lyst.com/lightfm/docs/home.html) [@kula-2015]\n",
    "python package. This hybrid recommender will utilize collaborative filtering in conjunction with user \n",
    "and item (news article) information. LightFM implements a \"light\" version of factorization machines, including only two-way interactions. \n",
    "\n",
    "# 2 Configure environment\n",
    "\n",
    "We'll require several Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e01af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e8d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Load python modules\"\n",
    "from math import floor, inf, log10, sqrt\n",
    "from os import sched_getaffinity\n",
    "from os.path import join as path_join\n",
    "from random import seed as set_seed, random, sample\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.preprocessing import normalize\n",
    "import itertools as it\n",
    "import json\n",
    "import lightfm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import scipy.sparse as sp\n",
    "import sys\n",
    "import tempfile\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30787752",
   "metadata": {},
   "source": [
    "Let's define a random seed for the sake of reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024bfa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Set seed\"\n",
    "seed = 1\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8bb34",
   "metadata": {},
   "source": [
    "# 3 Fetch data\n",
    "\n",
    "MIND is available to [download](https://msnews.github.io/) from Microsoft. It comprises a small variant and a large\n",
    "variant. The former is a small random sub-sample of the latter. Both\n",
    "variants each comprise a training set and a validation set and the large\n",
    "variant comprises in addition an unlabeled testing set. Since this\n",
    "official testing set is unlabeled, it won't be of much use to us, but\n",
    "we'll make do with the remaining subsets. We can retrieve the data by\n",
    "calling `fetch_mind()`, which is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Define function to fetch data\"\n",
    "def fetch_mind(\n",
    "    files=[\n",
    "        \"MINDlarge_dev.zip\",\n",
    "        \"MINDlarge_test.zip\",\n",
    "        \"MINDlarge_train.zip\",\n",
    "        \"MINDsmall_dev.zip\",\n",
    "        \"MINDsmall_train.zip\"\n",
    "    ],\n",
    "    root=\"https://mind201910small.blob.core.windows.net/release\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch Microsoft News Dataset from Internet.  In particular, download\n",
    "    compressed files and de-compress them into similarly named sub-directories\n",
    "    in the current directory.\n",
    "    \"\"\"\n",
    "    for f in files:\n",
    "        src = root + \"/\" + f\n",
    "        dstdir = f.rsplit(\".\", 1)[0]\n",
    "        r = requests.get(src, stream=True)\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            e = \"?\"\n",
    "            for s, n in requests.codes:\n",
    "                if r.status_code == n:\n",
    "                    e = s\n",
    "                    break\n",
    "            raise RuntimeError((\"failed to fetch file `{}'; \"\n",
    "                + \"server's response: {} ({})\").format(f, r.status_code, e))\n",
    "        try:\n",
    "            with tempfile.TemporaryFile() as tmpfil:\n",
    "                chunk_size = 2048\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    tmpfil.write(chunk)\n",
    "                with zipfile.ZipFile(tmpfil) as zipfil:\n",
    "                    zipfil.extractall(dstdir)\n",
    "        except OSError as e:\n",
    "            raise RuntimeError(\"failed to decompress file `{}': {}\".\n",
    "                format(f, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c2163a",
   "metadata": {},
   "source": [
    "For example, the following chunk if evaluated will attempt to fetch the\n",
    "small variant's training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d7cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Fetch data example\"\n",
    "\n",
    "fetch_mind([\n",
    "    \"MINDsmall_train.zip\",\n",
    "    \"MINDsmall_dev.zip\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af861ff",
   "metadata": {},
   "source": [
    "# 4 Prepare data\n",
    "\n",
    "Now that we have a local copy of MIND (or of a subset of it), let's\n",
    "process it for analysis.\n",
    "\n",
    "## Structure of MIND\n",
    "\n",
    "Each partition of MIND comprises:\n",
    "\n",
    "-   behaviors data (file named \"behaviors.tsv\")\n",
    "\n",
    "-   news data (\"news.tsv\")\n",
    "\n",
    "-   embeddings of named entities (\"entity_embedding.vec\")\n",
    "\n",
    "-   embeddings of relations of named entities\n",
    "    (\"relation_embedding.vec\")\n",
    "\n",
    "Detailed information about these data is available\n",
    "[elsewhere](https://github.com/msnews/msnews.github.io/blob/811c3da00f028ad7737d8c8e131770e04ffe6346/assets/doc/introduction.md),\n",
    "but a few details will be repeated here for the sake of this tutorial.\n",
    "\n",
    "### Behaviors data\n",
    "\n",
    "The behaviors dataset contains users' implicit feedback about news items in\n",
    "the form of history logs and impressions logs. MIND was\n",
    "assembled from click-activity that occurred over several weeks; the\n",
    "history log is a list of items that a user is known to have consumed\n",
    "during an initial portion this period. The impressions log is a list of\n",
    "items that were displayed to a user at a particular time afterward\n",
    "together with the user's implicit feedback about these items (1 = click, 0 = no click)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94168ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Set name of behaviors file\"\n",
    "behaviors = \"behaviors.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac332ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Define function to read behaviors data\"\n",
    "def get_behaviors(path):\n",
    "    \"\"\"Read behaviors data from file, return as DataFrame.\"\"\"\n",
    "    pdargs = {\n",
    "        \"sep\": \"\\t\",\n",
    "        \"header\": None,\n",
    "        \"names\": [\"impr_id\", \"user_id\", \"time\", \"history\", \"impr\"],\n",
    "        \"dtype\": {\n",
    "            \"impr_id\": str,\n",
    "            \"user_id\": str,\n",
    "        },\n",
    "        \"parse_dates\": [\"time\"],\n",
    "        \"converters\": {\n",
    "            \"history\": lambda x: x.split(),\n",
    "            \"impr\":    lambda s: [(item, int(label)) for x in s.split()\n",
    "                           for item, label in [x.rsplit(\"-\", 1)]],\n",
    "        },\n",
    "    }\n",
    "    with open(path) as f:\n",
    "        pdargs[\"filepath_or_buffer\"] = f\n",
    "        ret = pd.read_csv(**pdargs)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a5c821",
   "metadata": {},
   "source": [
    "### News data\n",
    "\n",
    "The news data is comprised of metadata about the news items that may be present\n",
    "in the history and impressions logs of the behavior data. Fields include\n",
    "category, sub-category, and named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e668312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Set name of news data file\"\n",
    "news = \"news.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0dba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Define function to read news data\"\n",
    "def get_news(path):\n",
    "    \"\"\"Read news data from file, return as DataFrame.\"\"\"\n",
    "    # pd.read_csv parses MINDsmall_train/news.tsv incorrectly, so we'll do it\n",
    "    # more manually.\n",
    "    cn = [\n",
    "        \"item\", \"cat\", \"subcat\", \"title\", \"abstract\", \"url\", \"ent_t\", \"ent_a\"\n",
    "    ]\n",
    "    conv = ([lambda x: x] * 6) + ([lambda x: json.loads(x)] * 2)\n",
    "    rec = []\n",
    "    with open(path) as f:\n",
    "        for lineno, line in enumerate(f):\n",
    "            x = line.strip().split(\"\\t\")\n",
    "            assert len(x) == len(conv),\\\n",
    "                \"expected {} fields, observed {} in record {}: `{}'\".\\\n",
    "                    format(len(cn), len(x), lineno, line)\n",
    "            try: rec.append([conv[i](xi) for i, xi in enumerate(x)])\n",
    "            except json.decoder.JSONDecodeError as e:\n",
    "                print(\"failed to decode JSON in record {}: {}\".\n",
    "                    format(lineno, e), file=sys.stderr)\n",
    "    return pd.DataFrame.from_records(rec, columns=cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92561428",
   "metadata": {},
   "source": [
    "### Entity-embeddings data\n",
    "\n",
    "The entity-embeddings dataset contains one-hundred dimensional embeddings of\n",
    "some of the named entities in the news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6443c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Set name of entity embeddings file\"\n",
    "entity_embeddings = \"entity_embedding.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513de838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Define function to read entity embeddings\"\n",
    "def get_embeddings(path):\n",
    "    \"\"\"Read embeddings data from file, return as DataFrame.\"\"\"\n",
    "    cn = [\"id\"] + [\"x\" + str(i) for i in range(100)]\n",
    "    conv = [lambda x: x] + ([lambda x: float(x)] * (len(cn) - 1))\n",
    "    rec = []\n",
    "    with open(path) as f:\n",
    "        for lineno, line in enumerate(f):\n",
    "            x = line.strip().split(\"\\t\")\n",
    "            assert len(x) == len(conv),\\\n",
    "                \"expected {} fields, observed {} in record {}: `{}'\".\\\n",
    "                    format(len(cn), len(x), lineno, line)\n",
    "            try: rec.append([conv[i](xi) for i, xi in enumerate(x)])\n",
    "            except ValueError as e:\n",
    "                print(\"failed to decode number in record {}: {}\".\n",
    "                    format(lineno, e), file=sys.stderr)\n",
    "    return pd.DataFrame.from_records(rec, columns=cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b771fd9",
   "metadata": {},
   "source": [
    "### Relation-embeddings data\n",
    "\n",
    "The relation-embeddings dataset contains one-hundred dimensional embeddings of\n",
    "knowledge-graph relations of some of the named entities in the news dataset. We\n",
    "won't do anything with them here.\n",
    "\n",
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Select data sets\"\n",
    "\n",
    "dir_training = \"MINDsmall_train\"\n",
    "dir_testing  = \"MINDsmall_dev\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d6685",
   "metadata": {},
   "source": [
    "Let's fetch our training set from the directory `dir_training` and our\n",
    "testing set from `dir_testing`. We'll allocate later a subset of the\n",
    "training set for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c308bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Fetch data\"\n",
    "data = {}\n",
    "dirs = {\n",
    "    \"tr\": dir_training,\n",
    "    \"te\": dir_testing,\n",
    "}\n",
    "for k, d in dirs.items():\n",
    "    data[k] = {}\n",
    "    data[k][\"beha\"] = get_behaviors(path_join(d, behaviors))\n",
    "    data[k][\"news\"] = get_news(path_join(d, news))\n",
    "    data[k][\"ents\"] = get_embeddings(path_join(d, entity_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1120ff9",
   "metadata": {},
   "source": [
    "Let's glimpse the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Glimpse of training set\"\n",
    "for k, v in data[\"tr\"].items():\n",
    "    print(\"{}:\\n\\tshape: {}\\n\\tcolumns: {}\".format(k, v.shape, v.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f557070",
   "metadata": {},
   "source": [
    "## Partition training set\n",
    "\n",
    "As mentioned previously, we'll partition the training set in two, reserving \n",
    "some data for validation. Our partitioning scheme will reserve\n",
    "for validation the training-set data from the most recent day on which\n",
    "training data was collected; the remainder will be retained for\n",
    "training during hyperparameter-tuning. In addition, we'll retain the\n",
    "complete training set for training a final model after having completed\n",
    "the hyperparameter-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0dc853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Inspect date range of original training set\"\n",
    "print(data[\"tr\"][\"beha\"][\"time\"].agg([min, max]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7cc2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Partition the training set\"\n",
    "\n",
    "# Initialize validation data from training data\n",
    "data[\"va\"] = dict(data[\"tr\"])\n",
    "\n",
    "# Copy full training set for training final model after tuning\n",
    "# hyper-parameters.\n",
    "data[\"tr_full\"] = dict(data[\"tr\"])\n",
    "\n",
    "# Reserve for validation the data associated with last day of training period.\n",
    "t0 = data[\"tr_full\"][\"beha\"][\"time\"].max() - pd.Timedelta(\"1 day\")\n",
    "p = data[\"tr_full\"][\"beha\"][\"time\"] <= t0\n",
    "data[\"tr\"][\"beha\"] = data[\"tr_full\"][\"beha\"].loc[p]\n",
    "data[\"va\"][\"beha\"] = data[\"tr_full\"][\"beha\"].loc[~p]\n",
    "for k in [\"tr\", \"va\"]:\n",
    "    print(\"{}-set behaviors table comprises {} records\".format(k,\n",
    "        data[k][\"beha\"].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461aadec",
   "metadata": {},
   "source": [
    "## Assemble interactions matrices\n",
    "\n",
    "For each subset of the data (training, validation, testing) we'll\n",
    "require a sparse matrix (in coordinate-list format) of user-item\n",
    "interactions. The rows correspond to users, columns correspond to\n",
    "items and entries specify the users' implicit feedback about the\n",
    "items; something like the following for each partition of the data-set,\n",
    "where ones indicate (implicit) positive feedback and zeros (implicit)\n",
    "negative feedback.\n",
    "\n",
    "            item_1 item_2 ... item_n\n",
    "    user_1 [     1      0 ...      1]\n",
    "    user_2 |     0      0 ...      1|\n",
    "     ...   |   ...    ... ...    ...|.\n",
    "    user_m [     1      1 ...      0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Create sparse matrices\"\n",
    "for k in data.keys():\n",
    "    users, items, ratings = [], [], []\n",
    "    for row in data[k][\"beha\"].itertuples():\n",
    "        for item, rating in row.impr:\n",
    "            users.append(row.user_id)\n",
    "            items.append(item)\n",
    "            ratings.append(rating)\n",
    "    data[k][\"users\"] = users\n",
    "    data[k][\"items\"] = items\n",
    "    data[k][\"ratings\"] = ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f09b4",
   "metadata": {},
   "source": [
    "The correspondence between users and rows and between items and columns\n",
    "must be the same for all partitions of the data-set, so that for example\n",
    "user $u$ is associated with row $r$ and item $i$ with column $c$\n",
    "irrespective of whether $(u,i)$ comes from the training set, from the\n",
    "validation set or from the testing set. To facilitate this, we'll create maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9220064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Create user map\"\n",
    "user_map = {}\n",
    "for v in data.values():\n",
    "    for x in v[\"users\"]:\n",
    "        _ = user_map.setdefault(x, len(user_map))\n",
    "print(\"{} users present in the data-set\".format(len(user_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5856e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Create item map\"\n",
    "item_map = {}\n",
    "for v in data.values():\n",
    "    for x in v[\"items\"]:\n",
    "        _ = item_map.setdefault(x, len(item_map))\n",
    "print(\"{} items present in the data-set\".format(len(item_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5689b",
   "metadata": {},
   "source": [
    "Having established these correspondences, we're ready to assemble the\n",
    "interactions matrices from the user-item-rating triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Assemble interaction matrices\"\n",
    "shape = (len(user_map), len(item_map))\n",
    "for k, v in data.items():\n",
    "    data[k][\"ui\"] = sp.coo_matrix((v[\"ratings\"], (\n",
    "        [user_map[x] for x in v[\"users\"]],\n",
    "        [item_map[x] for x in v[\"items\"]]\n",
    "    )), shape=shape)\n",
    "    print((\"{}-set interactions matrix has {} rows, {} columns and {} non-\"\n",
    "        + \"zero entries\").format(k, *data[k][\"ui\"].shape,\n",
    "        data[k][\"ui\"].count_nonzero()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e198c",
   "metadata": {},
   "source": [
    "We need not store explicit zeros in the training-set interactions matrix:\n",
    "Implicit zeros will suffice for training, because LightFM does its own\n",
    "sampling of implicit negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba5fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Eliminate zeros\"\n",
    "data[\"tr\"][\"ui\"].eliminate_zeros()\n",
    "data[\"tr_full\"][\"ui\"].eliminate_zeros()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c28a1",
   "metadata": {},
   "source": [
    "However, we do wish to retain any explicit zeros found in the validation\n",
    "and testing sets to retain the identity of the specific user-item pairs\n",
    "for which to make predictions during validation and during testing.\n",
    "\n",
    "## Assemble features matrices\n",
    "\n",
    "In addition to interactions data, LightFM can incorporate data about the\n",
    "interactions into the recommendation system; we'll call these additional\n",
    "data \"features\". In particular, we can incorporate user-features and\n",
    "item-features (a.k.a. side information) by passing LightFM separate sparse matrices whose rows\n",
    "correspond to users and to items respectively, whose columns correspond\n",
    "to features, and whose entries specify the users' and the items' values\n",
    "for these features.\n",
    "\n",
    "### User-features\n",
    "\n",
    "We'll start with user-features. Each partition of the data-set\n",
    "(training, validation, testing) will have a user-features matrix, which\n",
    "we'll organize in blocks (sub-matrices) for convenience. All blocks of a\n",
    "given partition will share the mapping of users to rows established for\n",
    "the interactions matrices, but each will have its own mapping of\n",
    "features to columns that will be shared with the corresponding blocks of\n",
    "the other partitions. After we've assembled the blocks, we'll stack them\n",
    "side-by-side to form the final user-features matrix.\n",
    "\n",
    "Here's a diagram. Suppose that our user-features matrix, `U`, comprises\n",
    "`p` features, `F1, F2, ..., Fp`. Then we might have\n",
    "\n",
    "    U = [F1 F2 ... Fp].\n",
    "\n",
    "Note that the feature blocks `F1, F2, ..., Fp` share the same number of\n",
    "rows: one per user. However, they may have different numbers of columns\n",
    "according to the underlying data that they represent; for example, a\n",
    "continuous feature `Fj` may be represented as a single column,\n",
    "\n",
    "         [x] user_1\n",
    "    Fj = |y|  ...   ,\n",
    "         [z] user_m\n",
    "\n",
    "whereas a categorical feature `Fk` of `g` levels will be one-hot encoded\n",
    "in as many columns,\n",
    "\n",
    "          level_1 level_2 ... level_g\n",
    "         [      1       0 ...       0] user_1\n",
    "    Fk = |      1       0 ...       0|  ...   .\n",
    "         [      0       1 ...       0] user_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Create dictionaries\"\n",
    "uft_blk = {k: {} for k in data.keys()}\n",
    "uft_map = {k: {} for k in data.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246c1ef",
   "metadata": {},
   "source": [
    "Specifically, in the user-features matrix we'll facilitate inclusion of\n",
    "the features named in `which_user_ft`. To use LightFM for traditional \n",
    "collaborative filtering (with no side information), specify\n",
    "`user` alone here, along with `item` alone later on in `which_item_ft`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Choose user features\"\n",
    "\n",
    "# Comment out any of the entries of `which_user_ft' to exclude these from the\n",
    "# features matrix.\n",
    "which_user_ft = [\n",
    "    \"user\",\n",
    "    \"history\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1172bcff",
   "metadata": {},
   "source": [
    "The identity-features block is merely the identity matrix that has a one\n",
    "for each user present in the data-set,\n",
    "\n",
    "            user_1 user_2 ... user_m\n",
    "    user_1 [     1     0  ...      0]\n",
    "    user_2 |     0     1  ...      0|\n",
    "     ...   |   ...   ...  ...    ...|\n",
    "    user_m [     0     0  ...      1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c33f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Assemble identity block of user-features matrices\"\n",
    "blk_name = \"user\"\n",
    "if blk_name in which_user_ft:\n",
    "    blk = sp.eye(len(user_map), format=\"coo\")\n",
    "    print(\"block `{}' has {} rows, {} columns, {} non-zero entries\".\n",
    "        format(blk_name, *blk.shape, blk.count_nonzero()))\n",
    "    for s in data:\n",
    "        uft_blk[s][blk_name] = blk\n",
    "        uft_map[s][blk_name] = user_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0d93f",
   "metadata": {},
   "source": [
    "The block associated with users' histories is only a little more\n",
    "complex. Some of the work that we'll do for this block we'll repeat for\n",
    "other features' blocks, so we'll first define a function for re-use. The\n",
    "purpose of this function is to ensure that the row- and column-mappings\n",
    "associated with the block under consideration are the same for all\n",
    "partitions of the data-set, so that for example the training set's\n",
    "history block associates the same rows and users and the same columns\n",
    "and items as the validation set's history block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b261c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Define function for getting feature blocks\"\n",
    "def get_ft_blk(data, get_ijv, row_map):\n",
    "    ijv = [get_ijv(x) for x in data]\n",
    "    col_map = {}\n",
    "    for j in it.chain.from_iterable(x[1] for x in ijv):\n",
    "        _ = col_map.setdefault(j, len(col_map))\n",
    "    blk = []\n",
    "    for i, j, v in ijv:\n",
    "        _i = [row_map[x] for x in i]\n",
    "        _j = [col_map[x] for x in j]\n",
    "        blk.append(sp.coo_matrix((v, (_i, _j)),\n",
    "            shape=(len(row_map), len(col_map))))\n",
    "    return (blk, col_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8177c",
   "metadata": {},
   "source": [
    "The rows of the history block will correspond to users and the columns\n",
    "to the items present in the users' histories. For each user `u`, the\n",
    "items present in the user's history log will receive a weight of one\n",
    "divided by the total number of items in the user's log---let there be\n",
    "`k_u` of them---so that each non-zero row of the block sums to one,\n",
    "\n",
    "            item_1         item_2 ...     item_n\n",
    "    user_1 [1/k_user_1          0 ... 1/k_user_1]\n",
    "    user_2 |         0          0 ... 1/k_user_2|\n",
    "     ...   |   ...            ... ...        ...|.\n",
    "    user_m [1/k_user_m 1/k_user_m ...          0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c3217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Assemble history block of user-features matrices\"\n",
    "blk_name = \"history\"\n",
    "if blk_name in which_user_ft:\n",
    "    def get_ijv(data):\n",
    "        I, J, V = [], [], []\n",
    "        for u, udat in data.groupby(\"user_id\"):\n",
    "            xs = set(it.chain.from_iterable(udat[\"history\"]))\n",
    "            I.extend(u for x in xs)\n",
    "            J.extend(xs)\n",
    "            V.extend(1/len(xs) for x in xs)\n",
    "        return (I, J, V)\n",
    "    blks, col_map = get_ft_blk(\n",
    "        data    = (x[\"beha\"] for x in data.values()),\n",
    "        get_ijv = get_ijv,\n",
    "        row_map = user_map\n",
    "    )\n",
    "    for s, b in zip(data.keys(), blks):\n",
    "        print((\"{}-set block `{}' has {} rows, {} columns, {} non-\"\n",
    "           + \"zero entries\").format(s, blk_name, *b.shape, b.count_nonzero()))\n",
    "        uft_blk[s][blk_name] = b\n",
    "        uft_map[s][blk_name] = col_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac10262d",
   "metadata": {},
   "source": [
    "We can now combine the blocks of the user-features matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071fbc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Combine blocks of the user-features\"\n",
    "for k in data.keys():\n",
    "    data[k][\"uft\"] = sp.hstack([x for x in uft_blk[k].values()], format=\"csr\")\n",
    "    print((\"{}-set user-features matrix has {} rows, {} columns, \"\n",
    "        + \"{} non-zero entries\").format(k, *data[k][\"uft\"].shape,\n",
    "        data[k][\"uft\"].count_nonzero()))\n",
    "    if data[k][\"uft\"].shape == (0, 0):\n",
    "        data[k][\"uft\"].shape = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51fa937",
   "metadata": {},
   "source": [
    "We might want to normalize the non-zero values of the matrices (as does\n",
    "`lightfm.Dataset.build_user_features`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92880e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Normalize non-zero values\"\n",
    "for k in data.keys():\n",
    "    if data[k][\"uft\"] is not None:\n",
    "        data[k][\"uft\"] = normalize(data[k][\"uft\"].tocsr(), norm=\"l1\",\n",
    "            copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1321c11",
   "metadata": {},
   "source": [
    "### Item-features\n",
    "\n",
    "Next come the item-features. Each partition of the data-set (training,\n",
    "validation, testing) will have an item-features matrix, which we'll\n",
    "organize in blocks like we've done with the user-features matrices. All\n",
    "blocks of a given partition will share the mapping of items to rows\n",
    "established between items and columns for the interactions matrices, but\n",
    "each will have its own mapping of features to columns that will be\n",
    "shared with the corresponding blocks of the other partitions. After\n",
    "we've assembled the blocks, we'll stack them side-by-side to form the\n",
    "final item-features matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Create dictionaries\"\n",
    "ift_blk = {k: {} for k in data.keys()}\n",
    "ift_map = {k: {} for k in data.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d9142",
   "metadata": {},
   "source": [
    "Specifically, in the item-features matrix we'll facilitate inclusion of\n",
    "the features named in `which_item_ft`. Note that this isn't an\n",
    "exhaustive list of the item-features that one might possibly include;\n",
    "consider, for example, that we could (but won't) incorporate the text of\n",
    "items' titles and abstracts with some more complex pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Choose item features\"\n",
    "\n",
    "# Comment out any of the entries of `which_item_ft' to exclude these from the\n",
    "# features matrix.\n",
    "which_item_ft = [\n",
    "    \"item\",\n",
    "    \"cat\",\n",
    "    \"subcat\",\n",
    "    \"label_ent_a\",\n",
    "    \"label_ent_t\",\n",
    "#    \"embedding_ent_a\",\n",
    "#    \"embedding_ent_t\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b13cc1",
   "metadata": {},
   "source": [
    "The identity-features block is merely the identity matrix that has a one\n",
    "for each item present in the data-set,\n",
    "\n",
    "            item_1 item_2 ... item_n\n",
    "    item_1 [     1     0  ...      0]\n",
    "    item_2 |     0     1  ...      0|\n",
    "     ...   |   ...   ...  ...    ...|\n",
    "    item_n [     0     0  ...      1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Assemble identity block of item-features matrices\"\n",
    "blk_name = \"item\"\n",
    "if blk_name in which_item_ft:\n",
    "    blk = sp.eye(len(item_map), format=\"coo\")\n",
    "    print(\"block `{}' has {} rows, {} columns, {} non-zero entries\".\n",
    "        format(blk_name, *blk.shape, blk.count_nonzero()))\n",
    "    for s in data:\n",
    "        ift_blk[s][blk_name] = blk\n",
    "        ift_map[s][blk_name] = item_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a2aed",
   "metadata": {},
   "source": [
    "Most of the item-features data resides in the news tables. These tables\n",
    "may comprise items that aren't present in any of the behaviors tables,\n",
    "which we may as well ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc3fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Delete data on irrelevant items\"\n",
    "for k, v in data.items():\n",
    "    data[k][\"news\"] = v[\"news\"].loc[\n",
    "        v[\"news\"][\"item\"].isin(item_map.keys())\n",
    "    ]\n",
    "    assert not any(data[k][\"news\"][\"item\"].duplicated()),\\\n",
    "        \"duplicate items in {}-set news data\".format(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf7226",
   "metadata": {},
   "source": [
    "The category and sub-category blocks will align items in rows (as\n",
    "previously stated) and their categories and sub-categories in columns.\n",
    "There's exactly one category and one sub-category associated with each\n",
    "item, each of which will be given a weight of one. Here follows a\n",
    "diagram of one possible such block, where we assume that there are `g`\n",
    "categories.\n",
    "\n",
    "            category_1 category_2 ... category_g\n",
    "    item_1 [         1          0 ...          0]\n",
    "    item_2 [         0          1 ...          0]\n",
    "      ...  |       ...        ... ...        ...|\n",
    "    item_n [         1          0 ...          0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad87a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Assemble category, sub-category blocks of item-features matrices\"\n",
    "for blk_name in [\"cat\", \"subcat\"]:\n",
    "    if blk_name not in which_item_ft:\n",
    "        continue\n",
    "    def get_ijv(x):\n",
    "        I, J, V = [], [], []\n",
    "        for row in x.itertuples():\n",
    "            I.append(row.item)\n",
    "            J.append(getattr(row, blk_name))\n",
    "            V.append(1)\n",
    "        return (I, J, V)\n",
    "    blks, col_map = get_ft_blk(\n",
    "        data    = (x[\"news\"] for x in data.values()),\n",
    "        get_ijv = get_ijv,\n",
    "        row_map = item_map\n",
    "    )\n",
    "    for s, b in zip(data.keys(), blks):\n",
    "        print((\"{}-set block `{}' has {} rows, {} columns, {} non-\"\n",
    "           + \"zero entries\").format(s, blk_name, *b.shape, b.count_nonzero()))\n",
    "        ift_blk[s][blk_name] = b\n",
    "        ift_map[s][blk_name] = col_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c531dc4f",
   "metadata": {},
   "source": [
    "Next, we'll assemble blocks for the named entities of items' titles and\n",
    "abstracts, in particular, for the associated labels (such as \"PGA Tour\"\n",
    "for the corresponding named entity). Note that there may be zero or more\n",
    "named entities per title/abstract per item, which we'll weight equally\n",
    "so that the weights of each item's named entities sum to one,\n",
    "\n",
    "            entity_1     entity_2 ...   entity_g\n",
    "    item_1 [1/k_item_1          0 ... 1/k_item_1]\n",
    "    item_2 |         0          0 ...          0|\n",
    "     ...   |   ...            ... ...        ...|\n",
    "    item_n [1/k_item_n 1/k_item_n ...          0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d52250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Assemble blocks for named entities from item titles and abstracts\"\n",
    "for k in [\"ent_t\", \"ent_a\"]:\n",
    "    blk_name = \"label_\" + k\n",
    "    if blk_name not in which_item_ft:\n",
    "        continue\n",
    "    def get_ijv(data):\n",
    "        I, J, V = [], [], []\n",
    "        for row in data.itertuples():\n",
    "            xs = set(x.get(\"Label\") for x in getattr(row, k))\n",
    "            xs.discard(None)\n",
    "            n = len(xs)\n",
    "            for x in xs:\n",
    "                I.append(row.item)\n",
    "                J.append(x)\n",
    "                V.append(1 / n)\n",
    "        return (I, J, V)\n",
    "    blks, col_map = get_ft_blk(\n",
    "        data    = (x[\"news\"] for x in data.values()),\n",
    "        get_ijv = get_ijv,\n",
    "        row_map = item_map\n",
    "    )\n",
    "    for s, b in zip(data.keys(), blks):\n",
    "        print((\"{}-set block `{}' has {} rows, {} columns, {} non-\"\n",
    "           + \"zero entries\").format(s, blk_name, *b.shape, b.count_nonzero()))\n",
    "        ift_blk[s][blk_name] = b\n",
    "        ift_map[s][blk_name] = col_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde2fd6",
   "metadata": {},
   "source": [
    "Next, we'll assemble blocks for the named entities' one-hundred\n",
    "dimensional embeddings. The blocks will associate each item with at most\n",
    "one embedding, although some items' named entities may lack embeddings\n",
    "and some items may be associated with more than one named entity. In the\n",
    "first case, the blocks' corresponding rows will be zero; in the second\n",
    "case, the embeddings will first be summed; and, in any case, the blocks'\n",
    "non-zero rows will be scaled to each have unit length,\n",
    "\n",
    "            embedding_1 embedding_2 ...   embedding_100\n",
    "    item_1 [          x           a ...               q]\n",
    "    item_2 |          y           b ...               r|\n",
    "     ...   |        ...         ... ...             ...|\n",
    "    item_n [          z           c ...               s]\n",
    "\n",
    "(In this diagram, the letters inside the matrix denote arbitrary real\n",
    "numbers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185232d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Assemble blocks for the one-hundred dimensional embeddings of named entities\"\n",
    "for k in [\"ent_t\", \"ent_a\"]:\n",
    "    blk_name = \"embedding_\" + k\n",
    "    if blk_name not in which_item_ft:\n",
    "        continue\n",
    "    def get_ijv(data):\n",
    "        I, J, V = [], [], []\n",
    "        news, ents = data[\"news\"], data[\"ents\"]\n",
    "        ents_by_item = news[[\"item\", k]].assign(\n",
    "            wdid=lambda row: [\n",
    "                list(filter(None, (x.get(\"WikidataId\") for x in xs)))\n",
    "                    for xs in row[k]\n",
    "            ]\n",
    "        ).explode(\"wdid\").merge(\n",
    "            ents, left_on=\"wdid\", right_on=\"id\"\n",
    "        ).drop(columns=[\"id\", \"wdid\", k]).groupby(\"item\").sum().transform(\n",
    "            lambda x: x / sqrt(x.dot(x)), axis=1\n",
    "        )\n",
    "        for ix, x in np.ndenumerate(ents_by_item.to_numpy()):\n",
    "            I.append(ents_by_item.index[ix[0]])\n",
    "            J.append(ents_by_item.columns[ix[1]])\n",
    "            V.append(x)\n",
    "        return (I, J, V)\n",
    "    blks, col_map = get_ft_blk(\n",
    "        data    = data.values(),\n",
    "        get_ijv = get_ijv,\n",
    "        row_map = item_map\n",
    "    )\n",
    "    for s, b in zip(data.keys(), blks):\n",
    "        print((\"{}-set block `{}' has {} rows, {} columns, {} non-\"\n",
    "           + \"zero entries\").format(s, blk_name, *b.shape, b.count_nonzero()))\n",
    "        ift_blk[s][blk_name] = b\n",
    "        ift_map[s][blk_name] = col_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb31b9",
   "metadata": {},
   "source": [
    "We can now combine the blocks of the item-features matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Combine blocks of item-features matrices\"\n",
    "for k in data.keys():\n",
    "    data[k][\"ift\"] = sp.hstack([x for x in ift_blk[k].values()], format=\"csr\")\n",
    "    print((\"{}-set item-features matrix has {} rows, {} columns, \"\n",
    "        + \"{} non-zero entries\").format(k, *data[k][\"ift\"].shape,\n",
    "        data[k][\"ift\"].count_nonzero()))\n",
    "    if data[k][\"ift\"].shape == (0, 0):\n",
    "        data[k][\"ift\"].shape = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a4e2f",
   "metadata": {},
   "source": [
    "We might want to normalize the non-zero values of the matrices (as does\n",
    "`lightfm.Dataset.build_item_features`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c4fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Normalize non-zero values\"\n",
    "for k in data.keys():\n",
    "    if data[k][\"ift\"] is not None:\n",
    "        data[k][\"ift\"] = normalize(data[k][\"ift\"].tocsr(), norm=\"l1\",\n",
    "            copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712db096",
   "metadata": {},
   "source": [
    "# 5 Train model\n",
    "\n",
    "We're ready to train a recommender by means of LightFM. There are many\n",
    "hyper-parameters to specify, such as the dimension of the latent feature\n",
    "space, the optimization criterion and the number of iterations of\n",
    "training. For simplicity, we'll leave many unspecified, which will\n",
    "therefore assume LightFM's default values. More information is available \n",
    "[here](https://making.lyst.com/lightfm/docs/lightfm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90823fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Set hyper-parameters\"\n",
    "hyper = {\n",
    "    \"dim\":      50,\n",
    "    \"loss\":     \"bpr\", # Bayesian personalized ranking\n",
    "    \"iter\":     100,\n",
    "    \"rate\":     0.05,\n",
    "    \"patience\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec0bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Define model\"\n",
    "model = lightfm.LightFM(\n",
    "    no_components = hyper[\"dim\"],\n",
    "    loss          = hyper[\"loss\"],\n",
    "    learning_rate = hyper[\"rate\"],\n",
    "    random_state  = seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd4abb6",
   "metadata": {},
   "source": [
    "We'll implement early-stopping to facilitate tuning. If this is enabled,\n",
    "training will cease early after `hyper[\"patience\"]` iterations of\n",
    "training that don't improve the recommender's best validation-set\n",
    "performance as per the stopping criterion `hyper[\"stop_crit\"]` (defined\n",
    "below). Note that training will likely take considerably longer if\n",
    "early-stopping is enabled.\n",
    "\n",
    "The LightFM module does not provide access to the loss function values, so\n",
    "early-stopping will be based on an accuracy-based metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8eaa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Define functions used for early stopping\"\n",
    "\n",
    "# For early-stopping based on normalized discounted cumulative gain\n",
    "def mean_ndcg(cond, pred):\n",
    "    return mean_row_stat(cond, pred, ndcg)\n",
    "\n",
    "# For early-stopping based on hit-rate at k\n",
    "def mean_hrk(cond, pred):\n",
    "    return mean_row_stat(cond, pred, hrk)\n",
    "\n",
    "# For early-stopping based on mean reciprocal rank\n",
    "def mean_recip_rank(cond, pred):\n",
    "    return mean_row_stat(cond, pred, recip_rank)\n",
    "\n",
    "# Function definition enabling collection of metrics\n",
    "def mean_row_stat(a, b, f):\n",
    "    \"\"\"\n",
    "    Return mean of `f' applied in parallel to corresponding non-zero rows\n",
    "    of LIL-format matrices `a' and `b'.\n",
    "    \"\"\"\n",
    "    with multiprocessing.Pool(n_avail_cpu()) as pool:\n",
    "        pairs = ((x, y) for x, y in zip(a.data, b.data) if len(x) and len(y))\n",
    "        return np.mean(pool.starmap(f, pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21152313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Setup for parallel processing\"\n",
    "def n_avail_cpu():\n",
    "    \"\"\"Return the number of central processing units available for use.\"\"\"\n",
    "    return len(sched_getaffinity(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d21c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Define evaluation metrics\"\n",
    "\n",
    "# Normalized Discounted Cumulative Gain\n",
    "def ndcg(cond, pred, k=5):\n",
    "    \"\"\"\n",
    "    Compute normalized discounted cumulative gain at k = `k' of list\n",
    "    of predictions `pred' given corresponding labels `cond'.\n",
    "    \"\"\"\n",
    "    a = np.array(cond).reshape(1, -1)\n",
    "    b = np.array(pred).reshape(1, -1)\n",
    "    return ndcg_score(a, b, k=k)\n",
    "\n",
    "# Hit rate at k\n",
    "def hrk(rel_true, rel_pred, k=5):\n",
    "    \"\"\"\n",
    "    Return hit-rate at `k', that is, the proportion of relevant items\n",
    "    (`rel_true' > 0) whose predicted relevance (`rel_pred') is among the\n",
    "    highest `k'.\n",
    "    \"\"\"\n",
    "    n = len(rel_true)\n",
    "    if len(rel_pred) != n:\n",
    "        raise ValueError(\"lists of relevance-values differ in length\")\n",
    "    # We shuffle the data to randomize the breaking of ties in ranks.\n",
    "    js = sample(range(n), k=n)\n",
    "    ps = [rel_true[j] > 0 for j in js]\n",
    "    qs = (r <= k for r in rankdata([-rel_pred[j] for j in js], \"ordinal\"))\n",
    "    den = sum(ps)\n",
    "    return inf if not den else sum(p & q for p, q in zip(ps, qs)) / den\n",
    "\n",
    "# Reciprocal rank\n",
    "def recip_rank(rel, pred):\n",
    "    \"\"\"\n",
    "    Return reciprocal rank of first relevant (`rel' > 0) element, where ranks\n",
    "    are awarded in order of non-increasing values of `pred' so that the lowest\n",
    "    rank is assigned to the highest value, the second-lowest rank to the\n",
    "    second-highest value and so on.\n",
    "    \"\"\"\n",
    "    n = len(rel)\n",
    "    if len(rel) != n:\n",
    "        raise ValueError(\"lists of relevance-values differ in length\")\n",
    "    # We shuffle the data to randomize the breaking of ties in ranks.\n",
    "    js = sample(range(n), k=n)\n",
    "    is_rel = [rel[j] > 0 for j in js]\n",
    "    ranks = rankdata([-pred[j] for j in js], \"ordinal\")\n",
    "    for p, r in zip(is_rel, ranks):\n",
    "        if p:\n",
    "            return 1 / r\n",
    "    return inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb680f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Run to enable early-stopping\"\n",
    "\n",
    "#hyper[\"stop_crit\"] = mean_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7a743",
   "metadata": {},
   "source": [
    "If early-stopping is disabled (above) then we'll merely train a recommender with\n",
    "the specified values of hyper-parameters on the full training set.\n",
    "Otherwise, we'll train on the reduced training set and evaluate the\n",
    "model after each epoch of training on the validation set until reaching\n",
    "the stopping point defined by `hyper[\"iter\"]` and by the early-stopping\n",
    "criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981558f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Fit model\"\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "if hyper.get(\"stop_crit\") is None:\n",
    "    # Train on full training set since we aren't tuning any hyper-parameters.\n",
    "    model.fit(\n",
    "        interactions  = data[\"tr_full\"][\"ui\"],\n",
    "        epochs        = hyper[\"iter\"],\n",
    "        user_features = data[\"tr_full\"][\"uft\"],\n",
    "        item_features = data[\"tr_full\"][\"ift\"],\n",
    "        num_threads   = n_avail_cpu()\n",
    "    )\n",
    "else:\n",
    "    stats = []\n",
    "    whichmax = 0\n",
    "    for i in range(hyper[\"iter\"]):\n",
    "        # Train for one epoch on one of the partitions of the training set.\n",
    "        model.fit_partial(\n",
    "            interactions  = data[\"tr\"][\"ui\"],\n",
    "            epochs        = 1,\n",
    "            user_features = data[\"tr\"][\"uft\"],\n",
    "            item_features = data[\"tr\"][\"ift\"],\n",
    "            num_threads   = n_avail_cpu()\n",
    "        )\n",
    "        # Evaluate model on other partition.\n",
    "        yhat = model.predict(\n",
    "            user_ids      = data[\"va\"][\"ui\"].row,\n",
    "            item_ids      = data[\"va\"][\"ui\"].col,\n",
    "            user_features = data[\"va\"][\"uft\"],\n",
    "            item_features = data[\"va\"][\"ift\"],\n",
    "            num_threads   = n_avail_cpu()\n",
    "        )\n",
    "        cond = sp.coo_matrix(data[\"va\"][\"ui\"])\n",
    "        pred = sp.coo_matrix((yhat, (cond.row, cond.col)), shape=cond.shape)\n",
    "        stat = hyper[\"stop_crit\"](cond.tolil(), pred.tolil())\n",
    "        print(\"validation-set statistic on iteration {:{}d}: {:f}\".\n",
    "            format(i, 1 + floor(log10(hyper[\"iter\"])), stat))\n",
    "        # Evaluate stopping criterion.\n",
    "        stats.append(stat)\n",
    "        if stat > stats[whichmax]:\n",
    "            whichmax =  i\n",
    "            continue\n",
    "        if i - whichmax >= hyper[\"patience\"]:\n",
    "            print(\"stopping early...best iteration: {}\".format(whichmax))\n",
    "            break\n",
    "    # Re-train on full training set for number of epochs adjusted according to\n",
    "    # difference in size between full training set and partial training set.\n",
    "    model.fit(\n",
    "        interactions  = data[\"tr_full\"][\"ui\"],\n",
    "        epochs        = int(1 + whichmax\n",
    "                            * data[\"tr_full\"][\"ui\"].count_nonzero()\n",
    "                            / data[\"tr\"][\"ui\"].count_nonzero()),\n",
    "        user_features = data[\"tr_full\"][\"uft\"],\n",
    "        item_features = data[\"tr_full\"][\"ift\"],\n",
    "        num_threads   = n_avail_cpu()\n",
    "    )\n",
    "    # Illustrate validation-set statistics.\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(stats, \"bo\")\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(hyper[\"stop_crit\"].__name__)\n",
    "    ax.set_title(\"validation-set statistics by training epoch\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cff6ab",
   "metadata": {},
   "source": [
    "# 6 Evaluate model\n",
    "\n",
    "Let's evaluate our trained recommender on the testing set. We'll compute\n",
    "the evaluation metrics in `metrics` (see below) from the recommender's\n",
    "predictions for the user-item pairs in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bfe895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Specify evaluation metrics\"\n",
    "\n",
    "metrics = {\n",
    "    \"mean HR@k\":            mean_hrk,\n",
    "    \"mean NDCG\":            mean_ndcg,\n",
    "    \"mean reciprocal rank\": mean_recip_rank,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Score testing set\"\n",
    "\n",
    "yhat = model.predict(\n",
    "    user_ids      = data[\"te\"][\"ui\"].row,\n",
    "    item_ids      = data[\"te\"][\"ui\"].col,\n",
    "    user_features = data[\"te\"][\"uft\"],\n",
    "    item_features = data[\"te\"][\"ift\"],\n",
    "    num_threads   = n_avail_cpu()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6045c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Evaluate model for all users\"\n",
    "\n",
    "# The implementations of the evaluation metrics in `metrics' each take two\n",
    "# LIL-format sparse user-item matrices as input: One assumed to provide\n",
    "# labels (`cond') and, the other (`pred'), predictions.\n",
    "cond = sp.coo_matrix(data[\"te\"][\"ui\"])\n",
    "pred = sp.coo_matrix((yhat, (cond.row, cond.col)), shape=cond.shape)\n",
    "# for all users\n",
    "_cond = cond.tolil()\n",
    "_pred = pred.tolil()\n",
    "print(\"for all users:\")\n",
    "for s, f in metrics.items():\n",
    "    print(\"\\t{}: {:f}\".format(s, f(_cond, _pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc9799",
   "metadata": {},
   "source": [
    "Note that there are users in the test set, which were not in the training set \n",
    "(a \"cold start\" situation). Below we consider two sets of users: (1) those that appear \n",
    "in both train and test, and (2) those that appear only in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efca0ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Evaluate model for users present both in training and testing sets\"\n",
    "\n",
    "# for users present both in training set and in testing set\n",
    "I = list(set(data[\"te\"][\"ui\"].row) & set(data[\"tr_full\"][\"ui\"].row))\n",
    "_cond = cond.tolil()[I]\n",
    "_pred = pred.tolil()[I]\n",
    "print(\"for users present both in training set and in testing set:\")\n",
    "for s, f in metrics.items():\n",
    "    print(\"\\t{}: {:f}\".format(s, f(_cond, _pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4274f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Evaluate model for testing-set users not present in training set\"\n",
    "\n",
    "# for testing-set users *not* present in training set\n",
    "I = list(set(data[\"te\"][\"ui\"].row) - set(data[\"tr_full\"][\"ui\"].row))\n",
    "_cond = cond.tolil()[I]\n",
    "_pred = pred.tolil()[I]\n",
    "print(\"for testing-set users not present in training set:\")\n",
    "for s, f in metrics.items():\n",
    "    print(\"\\t{}: {:f}\".format(s, f(_cond, _pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bbffc1",
   "metadata": {},
   "source": [
    "Note the (slight) difference in accuracy metrics between the two groups of users above.\n",
    "\n",
    "## Practice\n",
    "\n",
    "Try modifying the hyper-parameters (defined in the `hyper` dictionary in\n",
    "[Section 5](#5-Train-model)). Re-train the model and see how the accuracy metrics are\n",
    "affected.\n",
    "\n",
    "\n",
    "# 7 Compare to Baseline\n",
    "\n",
    "It might be informative to compare our recommender's performance to a\n",
    "simple baseline; for example, we might consider merely recommending\n",
    "items according to their popularity. We'll compute items' popularity\n",
    "from the training set such that the popularity of an item $i$ is the\n",
    "number of training-set interactions with $i$ divided by the total number\n",
    "of training-set interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208532e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Compute popularity of items for simple baseline\"\n",
    "\n",
    "item_pop = np.ravel(\n",
    "    data[\"tr_full\"][\"ui\"].sum(axis=0) / data[\"tr_full\"][\"ui\"].sum()\n",
    ")\n",
    "assert item_pop.shape[0] == len(item_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0910e2",
   "metadata": {},
   "source": [
    "Next we'll use these proportions to make recommendations for the testing\n",
    "set. This is easy: For each user-item pair $(u,i),$ the score of $(u,i)$\n",
    "is merely the popularity of $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7418e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Use popularity to make recommendations for test data\"\n",
    "yhat = item_pop[data[\"te\"][\"ui\"].col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e71e99f",
   "metadata": {},
   "source": [
    "Now we can evaluate this simple algorithm like we've done for LightFM to\n",
    "compare the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dfe4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Evaluate simple baseline on all users in test set\"\n",
    "cond = sp.coo_matrix(data[\"te\"][\"ui\"])\n",
    "pred = sp.coo_matrix((yhat, (cond.row, cond.col)), shape=cond.shape)\n",
    "_cond = cond.tolil()\n",
    "_pred = pred.tolil()\n",
    "for s, f in metrics.items():\n",
    "    print(\"{}: {:f}\".format(s, f(_cond, _pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860e7da0",
   "metadata": {},
   "source": [
    "Notice the difference in these accuracy metrics compared to the those associated\n",
    "with the LightFM model. \n",
    "\n",
    "\n",
    "\n",
    "# References\n",
    "\n",
    "Kula, Maciej. 2015. Metadata Embeddings for User and Item Cold-Start Recommendations. In Proceedings of the 2nd Workshop on New Trends on Content-Based Recommender Systems Co-Located with 9th ACM Conference on Recommender Systems (RecSys 2015), Vienna, Austria, September 16-20, 2015, edited by Toine Bogers and Marijn Koolen, 1448:1421. CEUR-WS.org. http://ceur-ws.org/Vol-1448/paper4.pdf.\n",
    "\n",
    "Wu, Fangzhao, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, et al. 2020. MIND: A Large-Scale Dataset for News Recommendation. In, 35973606. Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.331."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
