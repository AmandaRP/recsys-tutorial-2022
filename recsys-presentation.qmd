---
title: "RecSys Tutorial"
author: "Amanda Peterson"
format: 
  revealjs:
    scrollable: false
    chalkboard: true
    #theme: dark
    footer: "SCADS 2022 RecSys Tutorial"
    #logo: logo.png
    navigation-mode: linear
    transition: slide
    background-transition: fade
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##  {background-iframe="https://amanda.rbind.io/"}

## Outline

</br>

- Definitions
- Overview of "traditional" collaborative filtering models
- Deep learning for recommendation
- News Recommendations
- Hands-on exercise

## Types of Recommender Systems

::: {.panel-tabset}

### Collaborative Filtering 

Model patterns across all users' [feedback history]{.underline}.

-   **Pro**: Perform better than content filtering models when collaborative information is available.

-   **Con**: Unable to make recommendations in [cold start]{.underline} scenarios where little history is available for new users or items.

### Content Filtering

Individualized models focus on a single user's feedback history (or a group of like users) + metadata associated with items.

-   **Pro**: Able to handle new items.

-   **Cons**: Models for each user (or group) are fit in isolation. Require large amounts of data for each user.

### Hybrid

Collaborative + Content Filtering

-   **Pro**: Overcomes challenges of both types of filtering. User *and* item metadata (aka [side information]{.underline}) can be included.

:::

## More Definitions

**Explicit Feedback** **(or rating)**: Explicit rating given by the user for a product (e.g. star rating, thumbs up/down)

![](images/star_rating.jpg) 

**Implicit Feedback**: Actions taken by the user which can be used infer preferences about products (e.g. click data, dwell time, purchases)

## User-item Rating Array

![](images/user_item_matrix.png) 


## User-item Rating Array (Implicit)

![](images/user_item_matrix_implicit.png) 

## Data Sparsity

Data used to learn recommendations is notoriously sparse. 

Examples:

- 

## User-Item Array as a Bipartite Graph

## Dimension Reduction of the User-item Array

## Recommender Technology Timeline {transition="slide-in none-out" auto-animate=true}

![](images/nick_pentreath_timeline.png) 

[Presentation by Nick Pentreath at the 2018 Spark + AI Summit](https://youtu.be/y_TzOOCJqxI)

**2017 - 2022**: Advances in deep learning for recommender systems.

## {transition="none" auto-animate=true}

![](images/nick_pentreath_timeline.png){fig-align="center" width=70%} 

![](images/red_arrow.png){.absolute top=25 left=110 width="50" height="25"}

**Item-item similarity**: Can be computed using cosine or Jaccard similarity, for example.

![](images/user_item_matrix_item_sim.png){fig-align="center" width=30%}



## {transition="none" auto-animate=true}

![](images/nick_pentreath_timeline.png){fig-align="center" width=70%} 


![](images/red_arrow.png){.absolute top=150 left=220 width="50" height="25"}


![](images/netflix_fig2.png){fig-align="center" width=40%}

::: aside

[Matrix Facorization Techniques for Recommender Systems](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.147.8295&rep=rep1&type=pdf) by Koren, Bell, and Volinsky

:::


## {transition="none" auto-animate=true}


![](images/nick_pentreath_timeline.png){fig-align="center" width=70%}

![](images/red_arrow.png){.absolute top=25 left=360 width="50" height="25"}

![](images/factorization_machine.png){fig-align="center" width=75%}

# Deep Learning Models for Recommendation 

## Matrix Factorization as a Deep Learning Model

TODO: See Nick Petreath presentation

## Wide and Deep

:::: {.columns}

::: {.column width="60%"}
![](images/wide_and_deep.png){height=300 fig-alight='center'}
:::

::: {.column width="40%"}

- Model from Google 
- Example: 

:::
::::

::: aside

[Wide & Deep Learning for Recommender Systems](https://arxiv.org/pdf/1606.07792.pdf). 2016. Heng-Tze Cheng et. al. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (DLRS 2016). Association for Computing Machinery. 

:::



## NCF


:::: {.columns}

::: {.column width="50%"}
![](images/ncf.png){height=350}
:::

::: {.column width="50%"}
- Model for the 2017 TODO

:::

::::

::: aside

[Neural Collaborative Filtering](https://arxiv.org/pdf/1708.05031.pdf). 2017. Proceedings of the 26th International Conference on World Wide Web. Xiangnan He et. al.

:::



## DeepFM

:::: {.columns}

::: {.column width="60%"}
![](images/deepfm.png){height=350}
:::

::: {.column width="40%"}
- Model from Huawei
- Example: 

:::

::::

::: aside

[DeepFM: A Factorization-Machine based Neural Network for CTR Prediction](https://www.ijcai.org/proceedings/2017/0239.pdf). 2017. Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence.  (IJCAI-17). Huifeng Guo et. al.

:::


## DLRM

:::: {.columns}

::: {.column width="60%"}
![](https://gitlab.evoforge.org/arpete2/dlrs-catalog/-/raw/main/images/DLRM.png){height="350" fig-align="center"}
:::

::: {.column width="40%"}
Model from Facebook

:::

::::



::: aside
[Deep Learning Recommendation Model for Personalization and Recommendation Systems](https://arxiv.org/abs/1906.00091). 2019. arXiv. Maxim Naumov et. al. 
:::




## DCNV2

:::: {.columns}

::: {.column width="60%"}
![](images/dcnv2.png){height="400"}
:::

::: {.column width="40%"}
- Model from Google
- Cross Network: $x_{i+1} = x_0 (Wx_i + b) + x_i$

:::

::::

::: aside

[DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems](https://arxiv.org/pdf/2008.13535.pdf). 2021. In Proceedings of the Web Conference 2021 (WWW '21). Association for Computing Machinery. Ruoxi Wang et. al.

:::


## "Two tower" Sysetms

## Explainability of Deep Learning RecSys Models

![DCN-V2: Visualization of learned weight matrix in DCN-V2.
Rows and columns represents real features. For (a), feature
names were not shown for proprietary reasons; darker pixel
represents larger weight in its absolute value. For (b), each
block represents the Frobenius norm of each matrix block.](images/explain.png){height=300}

## Additional Reading: Explainability of Deep Learning RecSys Models

- paper


## Deep Learning for RecSys: Additional Reading


# Evaluation


## Evaluation Metrics for Recommendation Systems

![](images/evalrs.png){.absolute top=0 left=800 width="180" height="200"}

::: {.panel-tabset}


### Accuracy 

::: {.incremental}
  - Hit Rate at k (HR@k)
  - Mean Average Precision at k (MAP@k)
  - Normalized Discounted Cumulative Gain (NDCG@k)
  - Mean Percentile Rank (MPR@k)
  - MRR
:::
  
### Beyond accuracy

::: {.incremental}
  - Diversity at k
  - Novelty at k
  - Catalog coverage
:::

:::


# News Recommendation


# Other Flavors of RecSys Recommenders

- Recurrent Neural Networks
- Networks for Knowledge Graphs (diff than GNN?)


# Final Thoughts

## Tricks of the Trade

- **Ranking vs Retrieval**: If there are too many items to score, run a query to obtain a user's "candidate" recommended items. Use RecSys model to rank the candidates.
- **Sampling negative examples**: Weight items by popularity. This will better penalize popular items that can be recommended 


## Advice

*Are we really making much progress? A worrying analysis of recent neural recommendation approaches.*

- Make code/analysis/tuning/data reproducible. 
- Use as many baseline algorithms as possible (including non-NN), 
- Use as many metrics as possible (some are better than others for different tasks)


## Code 

- Microsoft GitHub Repo (framework?): https://github.com/microsoft/recommenders
- Tensorflow: 
- R: 
