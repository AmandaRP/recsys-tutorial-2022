---
title: "Tutorial:<br>Recommender Systems"
author: "Amanda Peterson"
date: May 20, 2022
format: 
  revealjs:
    scrollable: false
    chalkboard: true
    #theme: dark
    footer: "SCADS 2022 RecSys Tutorial"
    navigation-mode: linear
    transition: slide
    background-transition: fade
    #logo: "images/thumb.png"
#header-includes: \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Outline

</br>

- Definitions
- Overview of "traditional" collaborative filtering & hybrid models
- Deep learning for recommendation
- News recommendations
- Evaluation metrics
- Hands-on exercise

## Why Recommender Systems?

> "The most exciting, the most powerful artificial intelligence systems space
for the next couple of decades is recommender systems. They're going to have 
the biggest impact on our society because they affect how the information is
received, how we learn, what we think, how we communicate. These algorithms
are controlling us and we have to really think deeply as engineers about how
to speak up and think about their social implications." 

-[Lex Fridman](https://lexfridman.com/), MIT Deep
Learning and Artificial Intelligence Lectures: 
[Deep Learning State of the Art 2020](https://www.youtube.com/watch?v=0VH1Lim8gL8&list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf)

## Types of Recommender Systems

::: {.panel-tabset}

### Collaborative Filtering 

Model patterns across all users' [feedback history]{.underline}.

-   **Pro**: Perform better than content filtering models when collaborative information is available.

-   **Con**: Unable to make recommendations in [cold start]{.underline} scenarios where little history is available for new users or items.

### Content Filtering

Individualized models focus on a single user's feedback history (or a group of like users) + metadata associated with items.

-   **Pro**: Able to handle new items.

-   **Cons**: Models for each user (or group) are fit in isolation. Require large amounts of data for each user.

### Hybrid

Collaborative + Content Filtering

-   **Pro**: Overcomes challenges of both types of filtering. User *and* item metadata (aka [side information]{.underline}) can be included.

:::

## More Definitions

**Explicit Feedback** **(or rating)**: Explicit rating given by the user for a product (e.g. star rating, thumbs up/down)

![](images/star_rating.jpg) 

**Implicit Feedback**: Actions taken by the user which can be used infer preferences about products (e.g. click data, dwell time, purchases)

## User-item Rating Matrix {transition="slide-in none-out"}

![](images/user_item_matrix.png)


## User-item Rating Matrix (Implicit) {transition="none"}

![](images/user_item_matrix_implicit.png)

## Data Sparsity

Data used to learn recommendations is notoriously sparse. 

Examples:

- [MovieLens 25K](https://grouplens.org/datasets/movielens/): 99.75% sparse
- [Micrsoft News Dataset](https://msnews.github.io/): 99.98% sparse
- [Amazon](https://cseweb.ucsd.edu//~jmcauley/pdfs/www16a.pdf): > 99.99% sparse

## User-Item Matrix as a Bipartite Graph

![Items represented as blue nodes (bottom). Users represented as pink nodes (top). User-item interactions represented by an edge. Edges colored according to a particular user feature.](images/bipartite.png)

## Recommender Technology Timeline {transition="slide-in none-out" auto-animate=true}

![](images/nick_pentreath_timeline.png) 

[Presentation by Nick Pentreath at the 2018 Spark + AI Summit](https://youtu.be/y_TzOOCJqxI)

**2017 - 2022**: Advances in deep learning for recommender systems.

## {transition="none" auto-animate=true}

![](images/nick_pentreath_timeline.png){fig-align="center" width=70%} 

![](images/red_arrow.png){.absolute top=25 left=110 width="50" height="25"}

**Item-item similarity**: Can be computed using cosine or Jaccard similarity, for example.

![](images/user_item_matrix_item_sim.png){fig-align="center" width=30%}



## {transition="none" auto-animate=true}

![](images/nick_pentreath_timeline.png){fig-align="center" width=70%} 


![](images/red_arrow.png){.absolute top=150 left=220 width="50" height="25"}

:::: {.columns}

::: {.column width="35%"}

![](images/netflix_fig2.png){height=275}
:::

::: {.column width="65%"}

::: {.incremental}

<font size="5px">

- "Matrix Factorization", a.k.a. latent factor model

- **Goal**: Find user latent vector, $p_i \in R^f$, and item latent factor vector,
$q_i \in R^f$, for each user and item. User's rating of the $j^{th}$ item: 
$\hat{r}_{ij} = q_i^T p_i$

- **Possible Solution**: Compute the SVD of the user-item "matrix".

- **Issue**: How to impute missing values?

</font>

:::
:::
::::

::: aside

[Matrix Facorization Techniques for Recommender Systems](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.147.8295&rep=rep1&type=pdf). 2009. Koren, Bell, and Volinsky. IEEE COMPUTER.

:::




## {transition="none" auto-animate=true}

![](images/nick_pentreath_timeline.png){fig-align="center" width=70%} 


![](images/red_arrow.png){.absolute top=150 left=220 width="50" height="25"}

:::: {.columns}

::: {.column width="35%"}

![](images/netflix_fig2.png){height=275}
:::

::: {.column width="65%"}

<font size="5px">

- **Goal**: Find user latent vector, $p_i \in \mathbb{R}^f$, and item latent factor vector,
$q_i \in \mathbb{R}^f$, for each user and item. User's rating of the $j^{th}$ item: 
$\hat{r}_{ij} = q_i^T p_i$


- **Better Solution**: Model ratings and solve via stochastic gradient decent.

$$
min_{\substack{p,q}} \sum_{(i,j) \in \kappa} (r_{ij} - q_j^T p_i)^2 + \lambda (|| q_j ||^2 + || p_i ||^2)
$$

</font>

:::
::::

::: aside

[Matrix Facorization Techniques for Recommender Systems](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.147.8295&rep=rep1&type=pdf). 2009. Koren, Bell, and Volinsky. IEEE COMPUTER.

:::





## {transition="none" auto-animate=true}

![](images/nick_pentreath_timeline.png){fig-align="center" width=70%} 


![](images/red_arrow.png){.absolute top=150 left=220 width="50" height="25"}

:::: {.columns}

::: {.column width="35%"}

![](images/netflix_fig2.png){height=275}
:::

::: {.column width="65%"}

<font size="4px">


$$
min_{\substack{p,q}} \sum_{(i,j) \in \kappa} (r_{ij} - q_j^T p_i)^2 + \lambda (|| q_j ||^2 + || p_i ||^2)
$$

</font>
<font size="5px">

**Enhancements**:

- Incorporate global and main (user, item) effects
- Incorporate weights
- Model as a function of time
- Ensemble many models

</font>

:::
::::

::: aside

[Matrix Facorization Techniques for Recommender Systems](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.147.8295&rep=rep1&type=pdf). 2009. Koren, Bell, and Volinsky. IEEE COMPUTER.

:::









## {transition="none" auto-animate=true}


![](images/nick_pentreath_timeline.png){fig-align="center" width=70%}

![](images/red_arrow.png){.absolute top=25 left=360 width="50" height="25"}

:::: {.columns}

::: {.column width="60%"}


![](images/factorization_machine.png){fig-align="center" height=300}

:::
::: {.column width="40%"}

<font size="6px">
A hybrid model inspired by support vector machines (SVM) and factorization models.
</font>

<font size="5px">
$$
\hat{r}_{ij} = w_o + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n x_i x_j {\bf v}_i \cdot {\bf v}_j 
$$

</font>
:::
::::

::: aside
[Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf). 2010. IEEE International Conference on Data Mining. Steffen Rendle.
:::

## {transition="none" auto-animate=true}

![](images/nick_pentreath_timeline.png){fig-align="center" width=70%} 


![](images/red_arrow.png){.absolute top=150 left=510 width="50" height="25"}



**Example**: Pandora uses content associated with music that the listener has liked to recommend new music.

::: {.incremental}

- **Old**: "Musicologists" manually entered features about songs (mood, tempo, artist, etc.)
- **New**: Deep content models learn features from music audio

:::



## {transition="none" auto-animate=true}

![](images/nick_pentreath_timeline.png){fig-align="center" width=70%} 


![](images/red_arrow.png){.absolute top=25 left=685 width="50" height="25"}

<font size="6px">

- **2016-2017**: Deep Learning for Recommender Systems (DLRS) Workshop at RecSys Conferences

</font>

::: {.incremental}

<font size="6px">

- **By 2018** the DLRS workshop was discontinued in favor of presenting deep 
learning work in the main part of the conference.

- **Today**: State-of-the art deep learning for all types of recommender systems.

</font>

:::




## 

<br>

::: r-fit-text
**Deep Learning Models<br>for Recommender Systems** 
:::

## Matrix Factorization in a Deep Learning Framework

:::: {.columns}

::: {.column width="50%"}

![](images/matrix_factorization.png){fig-align="center"  height=350}
:::
::: {.column width="50%"}

::: {.incremental}
- Use sigmoid prediction function and binary cross-entropy (log loss) for binary feedback
- Use linear regression prediction function and MSE loss for 5-star ratings
:::

:::
::::

## Wide and Deep

:::: {.columns}

::: {.column width="60%"}
![](images/wide_and_deep.png){height=300 fig-alight='center'}
:::

::: {.column width="40%"}

- Model from Google 
- Memorization + Generalization
- Example: <font size="6px">App recommendation. Input: Item indices, user side & context info. Output: Binary (indicating app installation)</font>

:::
::::

::: aside

[Wide & Deep Learning for Recommender Systems](https://arxiv.org/pdf/1606.07792.pdf). 2016. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (DLRS 2016). Association for Computing Machinery. Heng-Tze Cheng et. al.

:::



## NCF


:::: {.columns}

::: {.column width="50%"}
![](images/ncf.png){height=350}
:::

::: {.column width="50%"}
- Previous RecSys Model for [MLPerf](https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/#:~:text=MLPerf%20is%20a%20consortium%20of,all%20conducted%20under%20prescribed%20conditions.)
- Collaborative filtering only (no side features)

:::

::::

::: aside

[Neural Collaborative Filtering](https://arxiv.org/pdf/1708.05031.pdf). 2017. Proceedings of the 26th International Conference on World Wide Web. Xiangnan He et. al.

:::



## DeepFM

:::: {.columns}

::: {.column width="60%"}
![](images/deepfm.png){height=350}
:::

::: {.column width="40%"}
- Model from Huawei
- Inspired by Factorization Machines
- Example: <font size="6px">[Criteo dataset](https://www.kaggle.com/competitions/criteo-display-ad-challenge/data) (13 integer features, 26 categorical features, binary labels.)</font>

:::

::::

::: aside

[DeepFM: A Factorization-Machine based Neural Network for CTR Prediction](https://www.ijcai.org/proceedings/2017/0239.pdf). 2017. Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence.  (IJCAI-17). Huifeng Guo et. al.

:::


## DLRM

:::: {.columns}

::: {.column width="60%"}
![](images/DLRM.png){height="350" fig-align="center"}
:::

::: {.column width="40%"}
- Model from Facebook
- Current [MLPerf](https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/#:~:text=MLPerf%20is%20a%20consortium%20of,all%20conducted%20under%20prescribed%20conditions.) RecSys model

:::

::::



::: aside
[Deep Learning Recommendation Model for Personalization and Recommendation Systems](https://arxiv.org/abs/1906.00091). 2019. arXiv. Maxim Naumov et. al. 
:::




## DCN-V2

:::: {.columns}

::: {.column width="60%"}
![](images/dcnv2.png){height="400"}
:::

::: {.column width="40%"}
- Model from Google
- Parallel and stacked versions
- Cross Network: $x_{i+1} = x_0 (Wx_i + b) + x_i$
- Claims to beat DLRM and DeepFM on [Criteo](https://www.kaggle.com/competitions/criteo-display-ad-challenge/data) and [MovieLens](https://grouplens.org/datasets/movielens) datasets

:::

::::

::: aside

[DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems](https://arxiv.org/pdf/2008.13535.pdf). 2021. In Proceedings of the Web Conference 2021 (WWW '21). Association for Computing Machinery. Ruoxi Wang et. al.

:::


## DCN-V2: Under the Hood 

We can consider the weight matrix W in the first cross
layer.

![DCN-V2: Visualization of learned weight matrix in DCN-V2.
Rows and columns represents real features. For (a), feature
names were not shown for proprietary reasons; darker pixel
represents larger weight in its absolute value. Orange boxes 
highlight notable feature crosses. For (b), each
block represents the Frobenius norm of each matrix block.](images/explain.png){height=300}

::: {.notes}
From the DCN-V2 paper: "The off-diagonal block corresponds to crosses that are known to be important, suggesting the effectiveness of DCN-V2. The diagonal 
block represents selfinteraction ($x^2$'s). Subplot (b) shows each block’s Frobenius norm and indicates some strong interactions learned, 
e.g., Gender x UserId, MovieId x UserId."
:::



## Terminology: Two-tower Sysetms

:::: {.columns}

::: {.column width="60%"}
![](images/two-tower.png){fig-align='center' height=400}

:::
::: {.column width="40%"}
<br>Used for item retreival (as opposed to ranking).
:::
::::

::: aside

Image source: [linkedin.com/pulse/personalized-recommendations-iv-two-tower-models-gaurav-chakravorty](https://www.linkedin.com/pulse/personalized-recommendations-iv-two-tower-models-gaurav-chakravorty). See also [Sampling-Bias-Corrected Neural Modeling for Large Corpus
Item Recommendations](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/6c8a86c981a62b0126a11896b7f6ae0dae4c3566.pdf). In Thirteenth
ACM Conference on Recommender Systems (RecSys 2019). Yi et al.

:::

## Additional Reading: Deep Learning for RecSys

<br><br>
[Deep Learning based Recommender System: A Survey and New Perspectives](https://arxiv.org/pdf/1707.07435.pdf). 2018. ACM Comput. Surv. 1, 1, Article 1. Zhang et al.

## 

<br><br>

::: r-fit-text
**News Recommendation**
:::

## MIND

<br><br>

We're working with the **Mi**crosoft **N**ews **D**ataset this summer! 

::: aside

[MIND: A Large-scale Dataset for News Recommendation](https://msnews.github.io/assets/doc/ACL2020_MIND.pdf). ACL 2020. Wu et al.

:::

## Challenges of News Recommendation {transition="slide-in none-out"}

(@) **Severe cold start problem**: New articles posted continuously. Usefulness of articles diminishes quickly. 

:::: {.columns}

::: {.column width="40%"}
![Survival time distribution of news articles](images/survival_mind.png){fig-align='left'}
:::

::: {.column width="60%"}
- Survival time of more than 84.5% news
articles is less than two days.
- Estimated using the time interval between
first and last appearance time of articles in the MIND dataset.
:::

::::

::: aside
Image sourced from the [MIND paper](https://msnews.github.io/assets/doc/ACL2020_MIND.pdf).
:::



## Challenges of News Recommendation {transition="none"}

(@) A user may get bored by a news feed with **lack of diversity**. Articles from a variety of topics may be more interesting.
(@) **Overly personalized** news feeds give can give a narrow view of the world.

The last point can be a result of over-emphasis on prediction accuracy. Beyond-accuracy metrics may help.


## NRMS

:::: {.columns}

::: {.column width="60%"}

![](images/nrms.png)
:::

::: {.column width="40%"}

* Model from Microsoft
* Best performing model in the [MIND paper](https://aclanthology.org/2020.acl-main.331.pdf).

:::
::::

::: aside
[Neural News Recommendation with Multi-Head Self-Attention](https://wuch15.github.io/paper/EMNLP2019-NRMS.pdf). 2019. Wue et. al.
:::


## Suggested Reading: News Recommendation

- [Technical Report](https://msnews.github.io/assets/doc/1.pdf) on Champion Solution for the [2020 MIND News Recommendation Challenge](https://msnews.github.io/competition.html)

- [News recommender system: a review of recent progress, challenges, and opportunities](https://doi.org/10.1007/s10462-021-10043-x). 2022. Artificial Intelligence Review. Raza, S., Ding, C. 


::: aside

:::


## 

<br><br><br>

::: r-fit-text
**Evaluation of RecSys Models**
:::

## Evaluation Metrics for Recommendation Systems

![](images/evalrs.png){.absolute top=0 left=800 width="180" height="200"}

::: {.panel-tabset}


### Accuracy 

- Hit Rate at $k$ (HR@$k$) 
- Mean Average Precision at $k$ (MAP@$k$)
- Normalized Discounted Cumulative Gain (NDCG@$k$) 
- Mean Reciprocal Rank (MRR) 


### Beyond accuracy

:::: {.columns}

::: {.column width="60%"}


- Diversity at k (Div@k)
- Novelty at k (Nov@k) 
- Catalog coverage (CC) 

::: 
::: {.column width="40%"}

::: {.callout-tip}

##

Unlike accuracy metrics, which are computed using a test set, 
beyond-accuracy metrics are computed on the final recommendation list.
:::
:::
::::

:::

## Hit Rate

<br>

::: {.callout-note icon=false}

## Hit rate at $k$

Percent of users who had a "positive" item in their top-$k$ recommendation list. 

:::

<br>
No penalization for location of positive(s) in list.



## Mean Average Precision

Contrary to HR@$k$, we penalize for "positive" items that are lower in the list (considering the top $k$ only).

::: {.callout-note icon=false}

## Average precision at $k$

$$
AP@k = \frac{1}{m} ∑_{i=1}^k P(i) * rel(i)
$$
where $P(i)$ is the precision at $i$ of the $i^{th}$ item, $rel(i)$ is the relevance of the $i^{th}$ item (0 or 1), and $m=min(k, \text{number of relevant items in full item space})$.  

:::

Calculate the mean over all users to obtain MAP@$k$.

::: aside
See [this blog post](https://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html) for a detailed overview.
:::

## NDCG

Similar to MAP@$k$, NDCG@$k$ also penalizes "positive" items that are lower on the recommendation list.

::: {.callout-note icon=false}

## Normalized Discounted Cumulative Gain at $k$

$$
NDCG@k = \frac{DCG_k}{IDCG_k}, \text{ where } DCG_k =  \sum_{i=1}^k \frac{rel(i)}{log_2(i+1)} \text{ and } IDCG_k = \sum_{i=1}^{|REL_k|} \frac{rel(i)}{log_2(i+1)}
$$
:::

REL$_k$ is the list of $k$ relevant items. The "I" in IDCG stands for "ideal." The above is computed for each user and then averaged over all users.


## Mean Reciprocal Rank

The mean across users of the reciprocal rank of their first relevant item.

::: {.callout-note icon=false}

## Mean Reciprocal Rank

Let $u$ be a user in the set of users $U$.  Let $r_u$ be the rank of the 
first relevant item for user $u$. Then,  
$$
MRR = \frac{1}{|U|}\sum_{u\in U} \frac{1}{r_u}.
$$

:::

## Beyond Accuracy: Catalog Coverage

<br>

::: {.callout-note icon=false}

## Catalog coverage at $k$

The percentage of the full set of items that are recommended to at least one user (considering users' top-$k$ recommendation lists).

:::

## Beyond Accuracy: Diversity

<br>

::: {.callout-note icon=false}

## Diversity at $k$

Aggregate (mean) dissimilarity between pairs of user's recommendation lists. 
<br><br>
Dissimilarity ($1 - \text{similarity}$) can be computed using metrics such as 
Jaccard similarity, Kendall's $\tau$, or Spearman's footrule.

:::

::: aside
Ronald Fagin, Ravi Kumar and D. Sivakumar. 2003. Comparing top k lists. SIAM J. Discrete Mathematics 17, 1, 134–160.
:::

## Beyond Accuracy: Novelty

<br>

::: {.callout-note icon=false}

## Novelty at $k$

Item novelty is the proportion of all users to whom the item is *not* recommended (considering all users' top-$k$ recommendation lists). Nov@$k$ is the mean item 
novelty over all users' top-$k$ items. 

:::


## 

<br>

::: {.callout-tip}

Use as many metrics as possible (some are better than others for different tasks)

:::


::: {.callout-warning}

Definitions and implementations of metrics can differ between software. 
It's important to use the same implementation when comparing models.

:::




## 

<br><br><br>

::: r-fit-text
Final Thoughts
:::

## Tips

- **Ranking vs retrieval**: If there are too many items to score, run a query to obtain a user's "candidate" recommended items. Use RecSys model to rank the candidates..
- **Sampling negative examples**: Weight items by popularity. This will better penalize popular items that can be recommended 

## More Tips

Personalized recommendations in conjunction with other types of recommendations can provide a more satisfying experience.

![](images/ui_sketch.png){fig-align='center'}

![](images/red_arrow.png){.absolute top=507 left=355 width="50" height="25"}



## Advice

*[Are we really making much progress? A worrying analysis of recent neural recommendation approaches.](https://dl.acm.org/doi/pdf/10.1145/3298689.3347058)*

- Make code/analysis/tuning/data reproducible. 
- Use as many baseline algorithms as possible (including non-NN)
- Use as many metrics as possible (some are better than others for different tasks)

::: aside
Paper published in the Thirteenth ACM Conference on Recommender Systems (2019) by Maurizio et al.
:::

## Additional Reading

<font size="6px">

**[Recommender Systems Paper Repository](https://github.com/hongleizhang/RSPapers)**

**Explainable Recommendations**

* [Measuring "Why" in Recommender Systems: a Comprehensive Survey on the Evaluation of Explainable Recommendation](https://arxiv.org/pdf/2202.06466.pdf). 2022. Chen, Zhang, and Wen.
    
**Knowledge graphs**

* [Deep Learning on Knowledge Graph for Recommender System: A Survey](https://arxiv.org/pdf/2004.00387.pdf). 2020. Association for Computing Machinery. Gao et al.

* [A Comprehensive Survey of Knowledge Graph-Based Recommender Systems: Technologies, Development,
and Contributions](https://doi.org/10.3390/info12060232). 2021. MDPI Information Journal. Chicaiza and Valdiviezo-Diaz.

</font>


## Code 

<br>

- Microsoft GitHub Repo: [https://github.com/microsoft/recommenders](https://github.com/microsoft/recommenders)
- Tensorflow: [https://www.tensorflow.org/recommenders](https://www.tensorflow.org/recommenders)
- R Keras: [https://github.com/AmandaRP](https://github.com/AmandaRP)

## 

::: {.r-fit-text}
Questions?
:::

<center>

Slides are available at: TODO

</center>