---
title: "Application of LightFM to Microsoft News Dataset"
date: now
date-format: "hh:mm:ss ZZ, dddd, MMMM D, YYYY"
execute: 
  eval: false
format:
  html:
    self-contained: true
    minimal: true
    toc: true
    toc-float: true
    number-sections: true
    code-copy: true
    code-fold: true
    theme: cosmo
#editor: visual
references:
- type: paper-conference
  id: kula-2015
  author:
  - family: Kula
    given: Maciej
  issued:
    year: 2015
  title: "Metadata embeddings for user and item cold-start recommendations"
  container-title: "Proceedings of the 2nd workshop on new trends on content-based recommender systems co-located with 9th ACM conference on recommender systems (RecSys 2015), Vienna, Austria, September 16-20, 2015"
  editor:
  - family: Bogers
    given: Toine
  - family: Koolen
    given: Marijn
  series: "CEUR workshop proceedings"
  volume: 1448
  page: 14-21
  publisher: "CEUR-WS.org"
  URL: "http://ceur-ws.org/Vol-1448/paper4.pdf"
- type: paper-conference
  id: microsoft-2020
  author:
    - family: Wu
      given: Fangzhao
    - family: Qiao
      given: Ying
    - family: Chen
      given: Jiun-Hung
    - family: Wu
      given: Chuhan
    - family: Qi
      given: Tao
    - family: Lian
      given: Jianxun
    - family: Liu
      given: Danyang
    - family: Xie
      given: Xing
    - family: Gao
      given: Jianfeng
    - family: Wu
      given: Winnie
    - family: Zhou
      given: Ming
  title: "MIND: A large-scale dataset for news recommendation"
  containter-title: "Proceedings of the 58th annual meeting of the association for computational linguistics"
  issued:
    month: jul
    year: 2020
  address: online
  publisher: "Association for Computational Linguistics"
  URL: "https://aclanthology.org/2020.acl-main.331"
  doi: "10.18653/v1/2020.acl-main.331"
  page: 3597-3606
editor_options: 
  markdown: 
    wrap: 72
---

# Introduction

Here we develop a recommender for the Microsoft News Dataset
[@microsoft-2020] by means of the
[LightFM](https://making.lyst.com/lightfm/docs/home.html) [@kula-2015]
python package.

# Configure environment

We'll require several Python modules.

```{python}
#| code-summary: "Load python modules"
from math import floor, inf, log10, sqrt
from os import sched_getaffinity
from os.path import join as path_join
from random import seed as set_seed, random, sample
from scipy.stats import rankdata
from sklearn.metrics import ndcg_score
import itertools as it
import json
import lightfm
import multiprocessing
import numpy as np
import pandas as pd
import requests
import scipy.sparse as sp
import sys
import tempfile
import zipfile
```

# Fetch data

MIND is available for download. It comprises a small variant and a large
variant. The former is a small random sub-sample of the latter. Both
variants each comprise a training set and a validation set. The large
variant additionally comprises an unlabeled testing set. Since this
official testing set is unlabeled, it won't be of much use to us, but
we'll make do with the remaining subsets. We can retrieve the data by
calling `fetch_mind`, which is defined below.

```{python}
#| code-summary: "Define function to fetch data"
def fetch_mind(
    files=[
        "MINDlarge_dev.zip",
        "MINDlarge_test.zip",
        "MINDlarge_train.zip",
        "MINDsmall_dev.zip",
        "MINDsmall_train.zip"
    ],
    root="https://mind201910small.blob.core.windows.net/release"
):
    """
    Fetch Microsoft News Dataset.  In particular, download compressed
    files and de-compress them into similarly named sub-directories
    in the current directory.
    """
    for f in files:
        src = root + "/" + f
        dstdir = f.rsplit(".", 1)[0]
        r = requests.get(src, stream=True)
        if r.status_code != requests.codes.ok:
            e = "?"
            for s, n in requests.codes:
                if r.status_code == n:
                    e = s
                    break
            raise RuntimeError(("failed to fetch file `{}'; "
                + "server's response: {} ({})").format(f, r.status_code, e))
        try:
            with tempfile.TemporaryFile() as tmpfil:
                chunk_size = 2048
                for chunk in r.iter_content(chunk_size=chunk_size):
                    tmpfil.write(chunk)
                with zipfile.ZipFile(tmpfil) as zipfil:
                    zipfil.extractall(dstdir)
        except OSError as e:
            raise RuntimeError("failed to decompress file `{}': {}".\
                format(f, e))
```

# Prepare data

Fetch the data:

```{python}
#| code-summary: "Fetch data"
fetch_mind()
```

Now that we have a local copy of MIND (or of a subset of it), let's
process it for analysis.

## Structure of MIND

Each official partition of MIND comprises:

-   behaviors data (file named "behaviors.tsv")
-   news data ("news.tsv")
-   embeddings of named entities ("entity_embedding.vec")
-   embeddings of relations of named entities
    ("relation_embedding.vec").

Detailed information about this data is available
[elsewhere](https://github.com/msnews/msnews.github.io/blob/811c3da00f028ad7737d8c8e131770e04ffe6346/assets/doc/introduction.md),
but a few details will be repeated here for the sake of this tutorial.

### Behaviors data

The behaviors data is comprised of users' implicit feedback about news
items in the form of history logs and impressions logs. MIND was
assembled from click-activity that occurred over several weeks and the
history log is a list of items that a user is known to have consumed
during an initial portion this period. The impressions log is a list of
items that were displayed to a user at a particular time (after the
training period) and the user's associated implicit feedback.

```{python}
#| code-summary: "Define function to read behaviors data"
behaviors = "behaviors.tsv"

def get_behaviors(path):
    """Read behaviors data from file, return as DataFrame."""
    def convert_impr(x):
        """for converting `impressions' column of behaviors data-set"""
        xs = x.split()
        ys = [None] * len(xs)
        for i, xi in enumerate(xs):
            y = xi.rsplit("-", 1)
            ys[i] = [y[0], int(y[1])]
        return ys
    pdargs = {
        "sep": "\t",
        "header": None,
        "names": ["impr_id", "user_id", "time", "history", "impr"],
        "dtype": {
            "impr_id": str,
            "user_id": str,
        },
        "parse_dates": ["time"],
        "converters": {
            "history": lambda x: x.split(),
            "impr":    convert_impr,
        },
    }
    with open(path) as f:
        pdargs["filepath_or_buffer"] = f
        ret = pd.read_csv(**pdargs)
    return ret
```

### News data

The news data contains metadata about the news items that may be present
in the behaviors data, such as category, sub-category and named
entities.

```{python}
#| code-summary: "Define function to read news data"
news = "news.tsv"

def get_news(path="news.tsv"):
    """Read news data from file, return as DataFrame."""
    # pd.read_csv parses MINDsmall_train/news.tsv incorrectly, so we'll do it
    # more manually.
    cn = [
        "item", "cat", "subcat", "title", "abstract", "url", "ent_t", "ent_a"
    ]
    conv = ([lambda x: x] * 6) + ([lambda x: json.loads(x)] * 2)
    rec = []
    with open(path) as f:
        for lineno, line in enumerate(f):
            x = line.strip().split("\t")
            assert len(x) == len(conv),\
                "expected {} fields, observed {} in record {}: `{}'".\
                    format(len(cn), len(x), lineno, line)
            try: rec.append([ conv[i](xi) for i, xi in enumerate(x) ])
            except json.decoder.JSONDecodeError as e:
                print("failed to decode JSON in record {}: {}".\
                    format(lineno, e), file=sys.stderr)
    return pd.DataFrame.from_records(rec, columns=cn)
```

### Entity-embeddings data

The entity-embeddings data are one-hundred dimensional embeddings of
some of the named entities in the news data.

```{python}
#| code-summary: "Define function to read entity embeddings"
entity_embeddings = "entity_embedding.vec"

def get_embeddings(path):
    """Read embeddings data from file, return as DataFrame."""
    cn = ["id"] + ["x" + str(i) for i in range(100)]
    conv = [lambda x: x] + ([lambda x: float(x)] * (len(cn) - 1))
    rec = []
    with open(path) as f:
        for lineno, line in enumerate(f):
            x = line.strip().split("\t")
            assert len(x) == len(conv),\
                "expected {} fields, observed {} in record {}: `{}'".\
                    format(len(cn), len(x), lineno, line)
            try: rec.append([ conv[i](xi) for i, xi in enumerate(x) ])
            except ValueError as e:
                print("failed to decode number in record {}: {}".\
                    format(lineno, e), file=sys.stderr)
    return pd.DataFrame.from_records(rec, columns=cn)
```

### Relation-embeddings data

The relation-embeddings data are one-hundred dimensional embeddings of
knowledge-graph relations of some of the named entities in the news
data. We won't do anything with them here.

## Read data

```{python}
#| code-summary: "Choose data set (or subset)"
dir_training   = "MINDsmall_train"
dir_validation = None
dir_testing    = None
```

Let's fetch our training set from the directory `dir_training`, our
validation data from `dir_validation` and our testing set from
`dir_testing`. If `dir_validation` is `None` then we'll allocate for
validation a random subset of the training set. If `dir_testing` is
`None` then we'll use the validation set for testing.

```{python}
#| code-summary: "Fetch data"
data = {}
dirs = {"tr": dir_training}
if dir_validation is not None:
    dirs["va"] = dir_validation
if dir_testing is not None:
    dirs["te"] = dir_testing
for k, d in dirs.items():
    data[k] = {}
    data[k]["beha"] = get_behaviors(path_join(d, behaviors))
    data[k]["news"] = get_news(path_join(d, news))
    data[k]["ents"] = get_embeddings(path_join(d, entity_embeddings))
```

Let's glimpse the training set.

```{python}
#| code-summary: "Glimpse training set"
for k, v in data["tr"].items():
    print("{}:\n\tshape: {}\n\tcolumns: {}".format(k, v.shape, v.columns))
```

As mentioned previously, we'll partition the training set, if a
validation set hasn't been supplied. Our partitioning scheme will
attempt to reserve for validation `100 * prob_va` percent of each user's
training data. When this percentage is less than one, we will randomly
choose the partition into which to put the data.

```{python}
#| code-summary: "Partition the training set"
seed = 1
set_seed(seed)
prob_va = 0.2
if "va" not in data:
    i_tr, i_va = [], []
    for u, udat in data["tr"]["beha"].groupby("user_id"):
        J = set(udat.index)
        assert len(J) == udat.shape[0],\
            "duplicate indices for user {}".format(u)
        # We want to put at least one observation on `u' into each
        # partition.  If this is impossible then randomly choose the
        # partition that receives the observation as per `prob_va'.
        k = round(prob_va * len(J))
        if k < 1:
            k = 1 if random() <= prob_va else 0
        j_va = sample(list(J), k=k)
        i_va.extend(j_va)
        i_tr.extend(J - set(j_va))
    data["va"] = {
        "beha":       data["tr"]["beha"].iloc[sorted(i_va)],
        "is_derived": True, # We'll use this later to avoid duplicate work.
    }
    data["tr"]["beha"] = data["tr"]["beha"].iloc[sorted(i_tr)]
print("{} rows in training set, {} in validation set".\
    format(data["tr"]["beha"].shape[0], data["va"]["beha"].shape[0]))
```

## Assemble interactions matrices

For each subset of the data-set we'll require a sparse matrix (in
coordinate-list format) of user-item interactions whose rows correspond
to users, whose columns correspond to items and whose entries specify
the users' implicit feedback about the items.

```{python}
#| code-summary: "Create sparse matrices"
for k in data:
    users, items, ratings = [], [], []
    for row in data[k]["beha"].itertuples():
        for item, rating in row.impr:
            users.append(row.user_id)
            items.append(item)
            ratings.append(rating)
    data[k]["users"] = users
    data[k]["items"] = items
    data[k]["ratings"] = ratings
```

After having identified all user-item-rating triples in the data-set, we
can now establish the correspondence between users and rows and between
items and columns.

```{python}
#| code-summary: "TODO"
user_map = {}
for v in data.values():
    for x in v["users"]:
        _ = user_map.setdefault(x, len(user_map))
print("{} users present in the data-set".format(len(user_map)))
```

```{python}
#| code-summary: "TODO"
item_map = {}
for v in data.values():
    for x in v["items"]:
        _ = item_map.setdefault(x, len(item_map))
print("{} items present in the data-set".format(len(item_map)))
```

Thus, we're ready to assemble the interactions matrices.

```{python}
#| code-summary: "Assemble interaction matrices"
shape = (len(user_map), len(item_map))
for k, v in data.items():
    data[k]["ui"] = sp.coo_matrix((v["ratings"], (
        [user_map[x] for x in v["users"]],
        [item_map[x] for x in v["items"]]
    )), shape=shape)
    print("{}-set interactions matrix has {} non-zero entries".\
        format(k, data[k]["ui"].count_nonzero()))
```

We needn't store explicit zeros in the training-set interactions matrix:
Implicit zeros will suffice for training, because lightfm does its own
sampling of implicit negatives.

```{python}
#| code-summary: "Eliminate zeros"
data["tr"]["ui"].eliminate_zeros()
```

However, we do wish to retain any explicit zeros found in the validation
and testing sets to retain the identity of the specific user-item pairs
for which to make predictions during validation and during testing.

## Assemble features matrices

In addition to interactions data, LightFM can incorporate data about the
interactions into the recommendation system; we'll call these additional
data "features". In particular, we can incorporate user-features and
item-features by passing lightfm separate sparse matrices whose rows
correspond to users and to items respectively, whose columns correspond
to features and whose entries specify the users' and the items' weights
on these features.

### User-features

We'll start with user-features. Each partition of the data-set will have
a user-features matrix, which we'll assemble by blocks for ease of
organization. All blocks of a given partition will share the mapping of
users to rows, but each will have its own mapping of features to columns
that will be shared with the corresponding blocks of the other
partitions.

```{python}
#| code-summary: "Create dictionaries"
uft_blk = {k: {} for k in data.keys()}
uft_map = {k: {} for k in data.keys()}
```

Specifically, in the user-features matrix we'll facilitate inclusion of
the features in `which_user_ft`.

```{python}
#| code-summary: "Choose user features"
which_user_ft = [
    "user",
    "history",
]
```

The identity-features block is merely the identity matrix that has a one
for each user present in the data-set.

```{python}
#| code-summary: "TODO"
blk_name = "user"
if blk_name in which_user_ft:
    blk = sp.eye(len(user_map), format="coo")
    print("block `{}' has {} rows, {} columns, {} non-zero entries".\
        format(blk_name, *blk.shape, blk.count_nonzero()))
    for s in data:
        uft_blk[s][blk_name] = blk
        uft_map[s][blk_name] = user_map
```

The block associated with users' histories is only a little more
complex. Some of the work that we'll do for this block we'll repeat for
other features' blocks, so we'll first define a function for re-use. The
purpose of this function is to ensure that the row- and column-mappings
associated with the block under consideration are the same for all
partitions of the data-set, so that for example the training set's
history block associates the same rows and users and the same columns
and items as the validation set's history block.

```{python}
#| code-summary: "TODO"
def get_ft_blk(data, get_ijv, row_map):
    ijv = [get_ijv(x) for x in data]
    col_map = {}
    for j in it.chain.from_iterable(x[1] for x in ijv):
        _ = col_map.setdefault(j, len(col_map))
    blk = []
    for i, j, v in ijv:
        _i = [row_map[x] for x in i]
        _j = [col_map[x] for x in j]
        blk.append(sp.coo_matrix((v, (_i, _j)),
            shape=(len(row_map), len(col_map))))
    return (blk, col_map)
```

The rows of the history block will correspond to users and the columns
to the items present in the users' histories. For each user, the items
present in the user's history log will receive a weight of one divided
by the number of items in the user's log so that each non-zero row of
the block sums to one.

```{python}
#| code-summary: "TODO"
blk_name = "history"
if blk_name in which_user_ft:
    def get_ijv(data):
        I, J, V = [], [], []
        for u, udat in data.groupby("user_id"):
            xs = set(it.chain.from_iterable(udat["history"]))
            I.extend(u for x in xs)
            J.extend(xs)
            V.extend(1/len(xs) for x in xs)
        return (I, J, V)
    # This bit here serves to avoid duplicate work: We needn't assemble
    # separate history blocks for partitions of the data-set that we've derived
    # from others (such as from the training set); instead, we'll later merely
    # store a duplicate reference to the appropriate block.
    beha = {k: v["beha"] for k, v in data.items() if "is_derived" not in v}
    blks, col_map = get_ft_blk(
        data    = beha.values(),
        get_ijv = get_ijv,
        row_map = user_map
    )
    for s, b in zip(beha.keys(), blks):
        print(("{}-set block `{}' has {} rows, {} columns, {} non-"
           + "zero entries").format(s, blk_name, *b.shape, b.count_nonzero()))
        uft_blk[s][blk_name] = b
        uft_map[s][blk_name] = col_map
```

We can now combine the blocks.

```{python}
#| code-summary: "Combine blocks"
for k in data.keys():
    data[k]["uft"] = sp.hstack([
        uft_blk[k].get(x, uft_blk["tr"][x]) for x in which_user_ft
    ], format="csr")
    print(("{}-set user-features matrix has {} rows, {} columns, "
        + "{} non-zero entries").format(k, *data[k]["uft"].shape,
        data[k]["uft"].count_nonzero()))
```

### Item-features

Next we process the item-features. Each partition of the data-set will
have an item-features matrix, which we'll assemble by blocks like we've
done with the user-features. All blocks of a given partition will share
the mapping of items to rows, but each will have its own mapping of
features to columns that will be shared with the corresponding blocks of
the other partitions.

```{python}
#| code-summary: "TODO"
ift_blk = {k: {} for k in data.keys()}
ift_map = {k: {} for k in data.keys()}
```

Specifically, in the item-features matrix we'll facilitate inclusion of
the features in `which_item_ft`.

```{python}
#| code-summary: "Choose item features"
which_item_ft = [
    "item",
    "cat",
    "subcat",
    "label_ent_a",
    "label_ent_t",
#    "embedding_ent_a",
#    "embedding_ent_t",
]
```

The identity-features block is merely the identity matrix that has a one
for each item present in the data-set.

```{python}
#| code-summary: "TODO"
blk_name = "item"
if blk_name in which_item_ft:
    blk = sp.eye(len(item_map), format="coo")
    print("block `{}' has {} rows, {} columns, {} non-zero entries".\
        format(blk_name, *blk.shape, blk.count_nonzero()))
    for s in data:
        ift_blk[s][blk_name] = blk
        ift_map[s][blk_name] = item_map
```

Most of the item-features data resides in the news tables. This table
may contain items that aren't present in any of the behaviors tables,
which we may as well ignore.

```{python}
#| code-summary: "TODO"
for k, v in data.items():
    if "news" in v:
        data[k]["news"] = v["news"].loc[
            v["news"]["item"].isin(item_map.keys())
        ]
        assert not any(data[k]["news"]["item"].duplicated()),\
            "duplicate items in {}-set news data".format(k)
```

The category and sub-category blocks will align items in rows and their
categories and sub-categories in columns. There's at exactly one
category and one sub-category associated with each item, each of which
will be given a weight of one.

```{python}
#| code-summary: "TODO"
for blk_name in ["cat", "subcat"]:
    if blk_name not in which_item_ft:
        continue
    def get_ijv(x):
        I, J, V = [], [], []
        for row in x.itertuples():
            I.append(row.item)
            J.append(getattr(row, blk_name))
            V.append(1)
        return (I, J, V)
    # This bit here serves to avoid duplicate work: We needn't assemble
    # separate blocks for partitions of the data-set that we've derived from
    # others (such as from the training set); instead, we'll later merely
    # store a duplicate reference to the appropriate block.
    news = {k: v["news"] for k, v in data.items() if "is_derived" not in v}
    blks, col_map = get_ft_blk(
        data    = news.values(),
        get_ijv = get_ijv,
        row_map = item_map
    )
    for s, b in zip(news.keys(), blks):
        print(("{}-set block `{}' has {} rows, {} columns, {} non-"
           + "zero entries").format(s, blk_name, *b.shape, b.count_nonzero()))
        ift_blk[s][blk_name] = b
        ift_map[s][blk_name] = col_map
```

Next, we'll assemble blocks for the named entities of items' titles and
abstracts, in particular, for the associated labels (such as "PGA Tour"
for the corresponding named entity). Note that there may be zero or more
named entities per title/abstract per item, which we'll weight equally
so that the weights of each item's named entities sum to one.

```{python}
#| code-summary: "TODO"
for k in ["ent_t", "ent_a"]:
    blk_name = "label_" + k
    if blk_name not in which_item_ft:
        continue
    def get_ijv(data):
        I, J, V = [], [], []
        for row in data.itertuples():
            xs = set(x.get("Label") for x in getattr(row, k))
            xs.discard(None)
            n = len(xs)
            for x in xs:
                I.append(row.item)
                J.append(x)
                V.append(1 / n)
        return (I, J, V)
    # This bit here serves to avoid duplicate work: We needn't assemble
    # separate blocks for partitions of the data-set that we've derived from
    # others (such as from the training set); instead, we'll later merely
    # store a duplicate reference to the appropriate block.
    news = {k: v["news"] for k, v in data.items() if "is_derived" not in v}
    blks, col_map = get_ft_blk(
        data    = news.values(),
        get_ijv = get_ijv,
        row_map = item_map
    )
    for s, b in zip(news.keys(), blks):
        print(("{}-set block `{}' has {} rows, {} columns, {} non-"
           + "zero entries").format(s, blk_name, *b.shape, b.count_nonzero()))
        ift_blk[s][blk_name] = b
        ift_map[s][blk_name] = col_map
```

Next, we'll assemble blocks for the named entities' one-hundred
dimensional embeddings. The blocks will associate each item with at most
one embedding, although some items' named entities may lack embeddings
and some items may be associated with more than one named entity. In the
first case, the blocks' corresponding rows will be zero; in the second
case, the embeddings will first be summed; and, in any case, the blocks'
non-zero rows will be scaled to each have unit length.

```{python}
#| code-summary: "TODO"
for k in ["ent_t", "ent_a"]:
    blk_name = "embedding_" + k
    if blk_name not in which_item_ft:
        continue
    def get_ijv(data):
        I, J, V = [], [], []
        news, ents = data["news"], data["ents"]
        for row in news.itertuples():
            ws = [x.get("WikidataId") for x in getattr(row, k)]
            p = ents["id"].isin(w for w in ws if w is not None)
            if sum(p) == 0:
                continue
            vs = ents.loc[p].drop(columns="id").sum(axis=0)
            vs /= sqrt(vs.pow(2).sum())
            for i, v in enumerate(vs):
                I.append(row.item)
                J.append(i)
                V.append(v)
        return (I, J, V)
    _data = {k: v for k, v in data.items() if "is_derived" not in v}
    blks, col_map = get_ft_blk(
        data    = _data.values(),
        get_ijv = get_ijv,
        row_map = item_map
    )
    for s, b in zip(_data.keys(), blks):
        print(("{}-set block `{}' has {} rows, {} columns, {} non-"
           + "zero entries").format(s, blk_name, *b.shape, b.count_nonzero()))
        ift_blk[s][blk_name] = b
        ift_map[s][blk_name] = col_map
```

We can now combine the blocks of item-features.

```{python}
#| code-summary: "Combine item feature blocks"
for k in data.keys():
    data[k]["ift"] = sp.hstack([
        ift_blk[k].get(x, ift_blk["tr"][x]) for x in which_item_ft
    ], format="csr")
    print(("{}-set item-features matrix has {} rows, {} columns, "
        + "{} non-zero entries").format(k, *data[k]["ift"].shape,
        data[k]["ift"].count_nonzero()))
```

# Learn mapping

We're ready to train a recommender by means of LightFM. There are many
hyper-parameters to specify, such as the dimension of the latent feature
space, the optimization criterion and the number of iterations of
training. For simplicity, we'll leave many unspecified, which will
therefore assume lightfm's default values.

```{python}
#| code-fold: show
#| code-summary: "Set hyper-parmaters"
hyper = {
    "dim":      15,
    "loss":     "warp-kos",
    "iter":     50,
    "patience": 5,
}
```

```{python}
#| code-fold: show
#| code-summary: "Define model"
model = lightfm.LightFM(
    no_components = hyper["dim"],
    loss          = hyper["loss"],
    random_state  = seed
)
```

We'll implement early-stopping to facilitate tuning. If this is enabled,
training will cease early after `hyper["patience"]` iterations of
training that don't improve the recommender's best validation-set
performance as per the stopping criterion `hyper["stop_crit"]`. Note
that training will likely take considerably longer if early-stopping is
enabled.

```{python}
#| code-fold: show
#| code-summary: "Setup for parallel processing"
def n_avail_cpu():
    """Return the number of central processing units available for use."""
    return len(sched_getaffinity(0))
```

```{python}
#| code-fold: show
#| code-summary: "Define evaluation metrics"

# for early-stopping based on normalized discounted cumulative gain
def mean_ndcg(cond, pred):
    return mean_row_stat(cond, pred, ndcg)

# for early-stopping based on hit-rate at k
def mean_hrk(cond, pred):
    return mean_row_stat(cond, pred, hrk)

# for early-stopping based on mean reciprocal rank
def mean_recip_rank(cond, pred):
    return mean_row_stat(cond, pred, recip_rank)

def mean_row_stat(a, b, f):
    """
    Return mean of `f' applied in parallel to corresponding non-zero rows
    of LIL-format matrices `a' and `b'.
    """
    with multiprocessing.Pool(n_avail_cpu()) as pool:
        pairs = ((x, y) for x, y in zip(a.data, b.data) if len(x) and len(y))
        return np.mean(pool.starmap(f, pairs))

def ndcg(cond, pred, k=5):
    """
    Compute normalized discounted cumulative gain at k = `k' of list
    of predictions `pred' given corresponding labels `cond'.
    """
    a = np.array(cond).reshape(1, -1)
    b = np.array(pred).reshape(1, -1)
    return ndcg_score(a, b, k=k)

def hrk(rel_true, rel_pred, k=5):
    """
    Return hit-rate at `k', that is, the proportion of relevant items
    (`rel_true' > 0) whose predicted relevance (`rel_pred') is among the
    highest `k'.
    """
    n = len(rel_true)
    if len(rel_pred) != n:
        raise ValueError("lists of relevance-values differ in length")
    # We shuffle the data to randomize the breaking of ties in ranks.
    js = sample(range(n), k=n)
    ps = [rel_true[j] > 0 for j in js]
    qs = (r <= k for r in rankdata([-rel_pred[j] for j in js], "ordinal"))
    den = sum(ps)
    return inf if not den else sum(p & q for p, q in zip(ps, qs)) / den

def recip_rank(rel, pred):
    """
    Return reciprocal rank of first relevant (`rel' > 0) element, where ranks
    are awarded in order of non-increasing values of `pred' so that the lowest
    rank is assigned to the highest value, the second-lowest rank to the
    second-highest value and so on.
    """
    n = len(rel)
    if len(rel) != n:
        raise ValueError("lists of relevance-values differ in length")
    # We shuffle the data to randomize the breaking of ties in ranks.
    js = sample(range(n), k=n)
    is_rel = [rel[j] > 0 for j in js]
    ranks = rankdata([-pred[j] for j in js], "ordinal")
    for p, r in zip(is_rel, ranks):
        if p:
            return 1 / r
    return inf
```

```{python}
#| code-fold: show
#| code-summary: "Run to enable early-stopping"
hyper["stop_crit"] = mean_ndcg
```

```{python}
#| code-fold: show
#| code-summary: "Fit model"
if hyper.get("stop_crit") is None:
    model.fit(
        interactions  = data["tr"]["ui"],
        epochs        = hyper["iter"],
        user_features = data["tr"]["uft"],
        item_features = data["tr"]["ift"],
        num_threads   = n_avail_cpu()
    )
else:
    stats = []
    whichmax = 0
    for i in range(hyper["iter"]):
        model.fit_partial(
            interactions  = data["tr"]["ui"],
            epochs        = 1,
            user_features = data["tr"]["uft"],
            item_features = data["tr"]["ift"],
            num_threads   = n_avail_cpu()
        )
        yhat = model.predict(
            user_ids      = data["va"]["ui"].row,
            item_ids      = data["va"]["ui"].col,
            user_features = data["va"]["uft"],
            item_features = data["va"]["ift"],
            num_threads   = n_avail_cpu()
        )
        cond = sp.coo_matrix(data["va"]["ui"])
        pred = sp.coo_matrix((yhat, (cond.row, cond.col)), shape=cond.shape)
        stat = hyper["stop_crit"](cond.tolil(), pred.tolil())
        print("validation-set statistic on iteration {:{}d}: {:f}".\
            format(i, 1 + floor(log10(hyper["iter"])), stat))
        stats.append(stat)
        if stat > stats[whichmax]:
            whichmax =  i
            continue
        if i - whichmax >= hyper["patience"]:
            print(("stopping early...best iteration: {}; "
                + "re-training accordingly...").format(whichmax))
            model.fit(
                interactions  = data["tr"]["ui"],
                epochs        = 1 + whichmax,
                user_features = data["tr"]["uft"],
                item_features = data["tr"]["ift"],
                num_threads   = n_avail_cpu()
            )
            break
```

# Evaluate mapping

Let's evaluate our trained recommender on the testing set.

```{python}
#| code-fold: show
#| code-summary: "Evaluate model on test data"
test = data.get("te", data["va"])
yhat = model.predict(
    user_ids      = test["ui"].row,
    item_ids      = test["ui"].col,
    user_features = test["uft"],
    item_features = test["ift"],
    num_threads   = n_avail_cpu()
)
cond = sp.coo_matrix(test["ui"])
pred = sp.coo_matrix((yhat, (cond.row, cond.col)), shape=cond.shape)
stats = {
    "mean HR@k":            mean_hrk,
    "mean NDCG":            mean_ndcg,
    "mean reciprocal rank": mean_recip_rank,
}
for s, f in stats.items():
    print("{}: {:f}".format(s, f(cond.tolil(), pred.tolil())))
```

# References
